{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter Sentiment Analysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkwHVyWvz1Pf",
        "colab_type": "text"
      },
      "source": [
        "# **Twitter Sentiment Analysis**\n",
        "\n",
        "# Problem Statement##\n",
        "\n",
        "The objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.\n",
        "\n",
        "## Data Description\n",
        "\n",
        "You are provided with the train and test set. Train set contains the tweets and their corresponding label which indicates whether the tweet is racist or not. You shou ld train the\n",
        "model using this training set. Here is the description about the files:\n",
        "\n",
        "1. train.csv - For training the models, we provide a labelled dataset of 31,962 tweets. The dataset is provided in the form of a csv file with each line storing a tweet id, its label and\n",
        "the tweet.\n",
        "2. test_tweets.csv - The test data file contains only tweet ids and the tweet text with each tweet in a new line.\n",
        "\n",
        "Your objective is to predict the labels of the given tweets in the test dataset.\n",
        "\n",
        "The training set contains three variables:\n",
        "1. id -> Unique identifier\n",
        "2. label -Y Binary target variable, where 1 denotes the tweet is racist/sexist and O denotes the tweet is not racist/sexist\n",
        "3. tweet text of the tweet\n",
        "\n",
        "The variable label is not present in the test set and you have to predict this.\n",
        "\n",
        "### Data Download\n",
        "\n",
        "You can download the dataset from the [Datahack Platform](https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/). Several Machine Learning and Deep\n",
        "Learning competitions are hosted on this platform. It gives you the following functionalities:\n",
        "\n",
        "-You can view and compare your score against that of other participants on the leaderboard.\n",
        "- You can also keep track of your submissions.\n",
        "\n",
        "## Approach to the Problem\n",
        "- Text Pre-processing: use word cloud to pre-process your data. This can include removing stop words, non important words, etc.\n",
        "- Create meta features: count of words, average word length, count of mentions, etc.\n",
        "- Train your model\n",
        "- Do the same pre-processing steps on test set as you did for the training set, and create the similar features\n",
        "- Generate predictions for the test set using the trained model\n",
        "- Save the predictions in a csv file (to check the format, refer to the sample submission file provided on the problem page)\n",
        "- Submit your predictions on the problem page and check your rank on the leaderboard.\n",
        "- To improve the score further, try out the advanced features like text representations: Bag of words (BOW) and tf-idf features\n",
        "- You can also use the word embedding to improve the performance of your model\n",
        "\n",
        "# Submission Details\n",
        "The following 3 files are to be uploaded.\n",
        "\n",
        "- test_predictions.csv - This should contain the 0/1 label for the tweets in test_tweets.csv, in the same order corresponding to the tweets in test_tweets.csv. Each 0/1 label should be in a new line.\n",
        "- A .zip file of source code - The code should produce the output file submitted and must be properly commented.\n",
        " \n",
        "\n",
        "Evaluation Metric:\n",
        "The metric used for evaluating the performance of classification model would be F1-Score.\n",
        "\n",
        "The metric can be understood as -\n",
        "\n",
        "**True Positives (TP)** - These are the correctly predicted positive values which means that the value of actual class is yes and the value of predicted class is also yes.\n",
        "\n",
        "**True Negatives (TN)** - These are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no.\n",
        "\n",
        "**False Positives (FP)** – When actual class is no and predicted class is yes.\n",
        "\n",
        "**False Negatives (FN)** – When actual class is yes but predicted class in no.\n",
        "\n",
        "**Precision** = TP/TP+FP\n",
        "\n",
        "**Recall** = TP/TP+FN\n",
        "\n",
        "**F1 Score** = 2*(Recall * Precision) / (Recall + Precision)\n",
        "\n",
        "F1 is usually more useful than accuracy, especially if for an uneven class distribution.\n"
      ]
    },
    {
      "source": [
        "# Get Data\n",
        "\n",
        "Downloaded from hackathon site.  https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/#ProblemStatement"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMKReYax0ITt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8fabe689-7b9f-418c-a16c-c7f49ca68de2"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA1TNplezy2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "#data =pd.read_csv('train_E6oV3lV.csv', index_col=0)\n",
        "train = pd.read_csv('train_E6oV3lV.csv', index_col=0)\n",
        "test = pd.read_csv('test_tweets_anuFYb8.csv', index_col=0)\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STp-e_hfqODO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ed413f69-921b-40fa-bc17-712d141f19d0"
      },
      "source": [
        "print(train.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(31962, 2)\n(17197, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQcpHViN05BT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "a4b31ca2-0054-42d5-9a3b-bea0f83cee53"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    label                                              tweet\n",
              "id                                                          \n",
              "1       0   @user when a father is dysfunctional and is s...\n",
              "2       0  @user @user thanks for #lyft credit i can't us...\n",
              "3       0                                bihday your majesty\n",
              "4       0  #model   i love u take with u all the time in ...\n",
              "5       0             factsguide: society now    #motivation"
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>tweet</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>@user when a father is dysfunctional and is s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>@user @user thanks for #lyft credit i can't us...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>bihday your majesty</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>#model   i love u take with u all the time in ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>factsguide: society now    #motivation</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WreGgZ-cxINF",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing\n",
        "\n",
        "Using pre-trained [VADER](https://predictivehacks.com/how-to-run-sentiment-analysis-in-python-using-vader/) which stands for **V**alence **A**ware **D**ictionary and s**E**ntiment **R**easoner and is specifically attuned to sentiments expressed in social media."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APCK5s_bgOd_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "966ad318-a9c1-41b9-e557-2689e6209d4b"
      },
      "source": [
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "sid = SentimentIntensityAnalyzer()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     C:\\Users\\czwea\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEeUFzkdyKZa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "c2383f2a-b840-4aad-c1e7-3d52d6145766"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\czwea\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MwoeBdy3UE9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "8ead7011-1da2-484f-829a-ab527d6f3b07"
      },
      "source": [
        "# print(stopwords.words('english'))"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQPnRCU8ytRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "# Tutorial about Python regular expressions: https://pymotw.com/2/re/\n",
        "stop_words = stopwords.words('english')\n",
        "def preprocessing(document):\n",
        "  document = list(document)\n",
        "  lemm = WordNetLemmatizer()\n",
        "  for i in range(0, len(document)):\n",
        "    document[i] = document[i].lower()\n",
        "    document[i] = document[i].replace('user','')\n",
        "    #regex to remove all the words with numbers for reference: https://stackoverflow.com/a/18082370/4084039\n",
        "    document[i] = re.sub(\"\\S*\\d\\S*\", \"\", document[i]).strip()\n",
        "    #remove special character: https://stackoverflow.com/a/5843547/4084039\n",
        "    document[i] = re.sub('[^A-Za-z0-9]+', ' ', document[i])\n",
        "    document[i] = word_tokenize(document[i])\n",
        "    document[i] = [lemm.lemmatize(words) for words in document[i]]\n",
        "    document[i] = ' '.join(e.lower() for e in document[i] if e.lower() not in stop_words)\n",
        "  return document"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "source": [
        "[Punkt Sentence Tokenizer](https://findanyanswer.com/what-is-nltk-punkt): This tokenizer divides a text into a list of sentences, by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences. \n",
        "\n",
        "[Wordnet](https://www.guru99.com/wordnet-nltk.html#:~:text=What%20is%20Wordnet%3F%20Wordnet%20is%20an%20NLTK%20corpus,it%20as%20a%20semantically%20oriented%20dictionary%20of%20English.): Wordnet is an NLTK corpus reader, a lexical database for English. It can be used to find the meaning of words, synonym or antonym. One can define it as a semantically oriented dictionary of English. Stats reveal that there are 155287 words and 117659 synonym sets included with English WordNet."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZLGksf77d0H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "161c23b7-0812-4dee-945e-a5b60e65ff35"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "preprocessed_train = preprocessing(train['tweet'])\n",
        "preprocessed_test = preprocessing(test['tweet'])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\czwea\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\czwea\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83efKnz2RgiW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f6b25e3b-c62a-423c-c4fe-87b1a09ef031"
      },
      "source": [
        "preprocessed_train[:20]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['father dysfunctional selfish drag kid dysfunction run',\n",
              " 'thanks lyft credit use cause offer wheelchair van pdx disapointed getthanked',\n",
              " 'bihday majesty',\n",
              " 'model love u take u time ur',\n",
              " 'factsguide society motivation',\n",
              " 'huge fan fare big talking leave chaos pay dispute get allshowandnogo',\n",
              " 'camping tomorrow danny',\n",
              " 'next school year year exam think school exam hate imagine actorslife revolutionschool girl',\n",
              " 'love land allin cavs champion cleveland clevelandcavaliers',\n",
              " 'welcome',\n",
              " 'ireland consumer price index mom climbed previous may blog silver gold forex',\n",
              " 'selfish orlando standwithorlando pulseshooting orlandoshooting biggerproblems selfish heabreaking value love',\n",
              " 'get see daddy today gettingfed',\n",
              " 'cnn call michigan middle school build wall chant tcot',\n",
              " 'comment australia opkillingbay seashepherd helpcovedolphins thecove helpcovedolphins',\n",
              " 'ouch junior junior yugyoem omg',\n",
              " 'thankful paner thankful positive',\n",
              " 'retweet agree',\n",
              " 'friday smile around via ig cooky make people',\n",
              " 'know essential oil made chemical']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kc-aiw0w8XtW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1ffcc516-211a-40f8-955e-006946616cbe"
      },
      "source": [
        "train.shape,len(preprocessed_train)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((31962, 2), 31962)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MU13ijb-9_u",
        "colab_type": "text"
      },
      "source": [
        "# Using Unigram, Bigram, Trigram and Tetragrams in TFIDF Vectorizer\n",
        "\n",
        "## Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train:  21414 x_test:  10548\ny_train:  21414 y_test:  10548\n"
          ]
        }
      ],
      "source": [
        "y = train['label']\n",
        "x = preprocessed_train\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test=train_test_split(x, y, test_size = 0.33, stratify = y)\n",
        "\n",
        "\n",
        "print(\"x_train: \", len(x_train), \"x_test: \", len(x_test))\n",
        "print(\"y_train: \", len(y_train), \"y_test: \", len(y_test))"
      ]
    },
    {
      "source": [
        "Here is a [good resource](https://towardsdatascience.com/hacking-scikit-learns-vectorizers-9ef26a7170af) to understand the following code.\n",
        "\n",
        "Of course this helps too:  [sklearn.feature_extraction.text.TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(21414, 26737)   (0, 22487)\t0.5201568246294398\n  (0, 19633)\t0.4996381355838922\n  (0, 17443)\t0.4036337833633234\n  (0, 8255)\t0.2856149096331439\n  (0, 5826)\t0.48507989402234686\n  (1, 25824)\t0.3969471248342019\n  (1, 25386)\t0.3212593168195984\n  (1, 20676)\t0.3136877969947271\n  (1, 16037)\t0.30271066612297354\n  (1, 15457)\t0.4323985404371476\n  (1, 7565)\t0.38462077497162966\n  (1, 2486)\t0.46291469166844706\n  (2, 25500)\t0.2914579153080008\n  (2, 20264)\t0.413116731168469\n  (2, 10410)\t0.28702971832106644\n  (2, 10397)\t0.35128122191329086\n  (2, 10392)\t0.4257084470340827\n  (2, 683)\t0.3255745422994896\n  (2, 153)\t0.5013739417789232\n  (3, 26482)\t0.30880945027975465\n  (3, 19648)\t0.4988434140669818\n  (3, 10276)\t0.48430832875992014\n  (3, 8011)\t0.3483337528784555\n  (3, 4150)\t0.30503718987935996\n  (3, 2839)\t0.3124634387836758\n  :\t:\n  (17194, 23467)\t0.353208750696606\n  (17194, 20464)\t0.19644450719976192\n  (17194, 16838)\t0.2353022622010997\n  (17194, 16764)\t0.3139065878647706\n  (17194, 16016)\t0.19144461171927224\n  (17194, 13509)\t0.15714234436792646\n  (17194, 10629)\t0.26532057542844534\n  (17194, 4445)\t0.2819308161864709\n  (17194, 1285)\t0.32289720328402854\n  (17194, 792)\t0.2872216524315281\n  (17195, 26185)\t0.42329310141336013\n  (17195, 19840)\t0.22366055529345863\n  (17195, 17032)\t0.2958130255347216\n  (17195, 15033)\t0.577968360422166\n  (17195, 13218)\t0.283111555467857\n  (17195, 10219)\t0.16661079097999235\n  (17195, 6126)\t0.2904086816727896\n  (17195, 5410)\t0.27960796291995815\n  (17195, 4832)\t0.28074367445590526\n  (17196, 21783)\t0.33092718334460003\n  (17196, 16133)\t0.5099215576678672\n  (17196, 16114)\t0.4741226828032138\n  (17196, 9432)\t0.3724072589602923\n  (17196, 8735)\t0.30051010630996483\n  (17196, 6638)\t0.4203348611285103\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#Unigram features\n",
        "vectorizer_unigram = TfidfVectorizer()\n",
        "vectorizer_unigram.fit(x_train)\n",
        "tfidf_words = vectorizer_unigram.get_feature_names()\n",
        "x_tfidf = vectorizer_unigram.transform(x)\n",
        "x_train_tfidf = vectorizer_unigram.transform(x_train)\n",
        "x_test_tfidf = vectorizer_unigram.transform(x_test)\n",
        "x_CV_tfidf = vectorizer_unigram.transform(preprocessed_test)\n",
        "print(x_train_tfidf.shape)"
      ]
    },
    {
      "source": [
        "Bigram Features"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(21414, 131724)\n"
          ]
        }
      ],
      "source": [
        "# min_dffloat or int, default=1\n",
        "# When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float in range of [0.0, 1.0], the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
        "\n",
        "vectorizer_bigram = TfidfVectorizer(ngram_range = (1, 2), min_df = 1)\n",
        "vectorizer_bigram.fit(x_train)\n",
        "tfidf_bigram_words = vectorizer_bigram.get_feature_names()\n",
        "x_tfidf_bigram = vectorizer_bigram.transform(x)\n",
        "x_train_tfidf_bigram = vectorizer_bigram.transform(x_train)\n",
        "x_test_tfidf_bigram = vectorizer_bigram.transform(x_test)\n",
        "x_CV_tfidf_bigram = vectorizer_bigram.transform(preprocessed_test)\n",
        "print(x_train_tfidf_bigram.shape)"
      ]
    },
    {
      "source": [
        "Trigram Features"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(21414, 28802)\n"
          ]
        }
      ],
      "source": [
        "#Trigram features\n",
        "vectorizer_trigram = TfidfVectorizer(ngram_range = (1,3), min_df = 2)\n",
        "vectorizer_trigram.fit(x_train)\n",
        "x_train_tfidf_trigram = vectorizer_trigram.transform(x_train)\n",
        "x_test_tfidf_trigram = vectorizer_trigram.transform(x_test)\n",
        "x_CV_tfidf_trigram =vectorizer_trigram.transform(preprocessed_test)\n",
        "print(x_train_tfidf_trigram.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Tetragram Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(21414, 33344)\n"
          ]
        }
      ],
      "source": [
        "#Tetragram features\n",
        "vectorizer_tetragram = TfidfVectorizer(ngram_range = (1, 4), min_df = 2)\n",
        "vectorizer_tetragram.fit(x_train)\n",
        "x_train_tfidf_tetragram = vectorizer_tetragram.transform(x_train)\n",
        "x_test_tfidf_tetragram = vectorizer_tetragram.transform(x_test)\n",
        "x_CV_tfidf_tetragram = vectorizer_tetragram.transform(preprocessed_test)\n",
        "print(x_train_tfidf_tetragram.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Pentagram Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(21414, 36850)\n"
          ]
        }
      ],
      "source": [
        "#Pentagram features\n",
        "vectorizer_pentagram = TfidfVectorizer(ngram_range = (1, 5), min_df = 2)\n",
        "vectorizer_pentagram.fit(x_train)\n",
        "x_train_tfidf_pentagram = vectorizer_pentagram.transform(x_train)\n",
        "x_test_tfidf_pentagram = vectorizer_pentagram.transform(x_test)\n",
        "x_CV_tfidf_pentagram = vectorizer_pentagram.transform(preprocessed_test)\n",
        "print(x_train_tfidf_pentagram.shape)"
      ]
    },
    {
      "source": [
        "Polarity scores come from VADER\n",
        "\n",
        "Recall this was previously imported:\n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "Recalling 'x':"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['father dysfunctional selfish drag kid dysfunction run',\n",
              " 'thanks lyft credit use cause offer wheelchair van pdx disapointed getthanked',\n",
              " 'bihday majesty',\n",
              " 'model love u take u time ur',\n",
              " 'factsguide society motivation',\n",
              " 'huge fan fare big talking leave chaos pay dispute get allshowandnogo',\n",
              " 'camping tomorrow danny',\n",
              " 'next school year year exam think school exam hate imagine actorslife revolutionschool girl',\n",
              " 'love land allin cavs champion cleveland clevelandcavaliers',\n",
              " 'welcome']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "x[: 10]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeZfD88DhJ1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ss_tr = []\n",
        "ss_data = []\n",
        "ss_test = []\n",
        "ss_cv = []\n",
        "\n",
        "for i in x:\n",
        "    s = sid.polarity_scores(i)\n",
        "    ss_data.append(list(s.values()))\n",
        "\n",
        "for i in x_train:\n",
        "    s = sid.polarity_scores(i)\n",
        "    ss_tr.append(list(s.values()))\n",
        "\n",
        "for i in x_test:\n",
        "    s=sid.polarity_scores(i)\n",
        "    ss_test.append(list(s.values()))\n",
        "\n",
        "for i in preprocessed_test:\n",
        "    s = sid.polarity_scores(i)\n",
        "    ss_cv.append(list(s.values()))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvbt-1q98LHU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4e44225d-d2b3-40b0-9cc2-5fdfed7d1226"
      },
      "source": [
        "from scipy.sparse import hstack\n",
        "#X_test_grade\n",
        "import scipy\n",
        "data = hstack((x_tfidf, x_tfidf_bigram, ss_data)).tocsr()\n",
        "x_tr = hstack((x_train_tfidf, x_train_tfidf_bigram, ss_tr)).tocsr()\n",
        "x_cv = hstack((x_CV_tfidf, x_CV_tfidf_bigram, ss_cv)).tocsr()\n",
        "x_tst=hstack((x_test_tfidf, x_test_tfidf_bigram, ss_test)).tocsr()\n",
        "print(data.shape, x_tr.shape, x_tst.shape, x_cv.shape)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(31962, 158465) (21414, 158465) (10548, 158465) (17197, 158465)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYR_Mk9MD6OK",
        "colab_type": "text"
      },
      "source": [
        "**Using LinearSVC Algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xh2mJOBS8uES",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "outputId": "fa3a82d3-89b5-4a29-e7cc-0f56259bc4fb"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import roc_auc_score,f1_score,accuracy_score\n",
        "C=[0.001,0.01,0.1,0.5,1,10,100,1000]\n",
        "for i in C:\n",
        "  model = LinearSVC(penalty='l2', C=i, dual=False, random_state=0, max_iter=1000)\n",
        "  model.fit(x_tr, y_train)\n",
        "  y_pred_tr = model.predict(x_tr)\n",
        "  y_pred_te = model.predict(x_tst)\n",
        "  print(\"C: {}\".format(i))\n",
        "  print('Train Accuracy:', accuracy_score(y_pred_tr, y_train))\n",
        "  print(\"Train F1 Score: \", f1_score(y_pred_tr, y_train))\n",
        "  print('Test Accuracy:', accuracy_score(y_pred_te, y_test))\n",
        "  print(\"Test F1 Score: \", f1_score(y_pred_te, y_test))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C: 0.001\n",
            "Train Accuracy: 0.929858970766788\n",
            "Train F1 Score:  0.0\n",
            "Test Accuracy: 0.9298445202882063\n",
            "Test F1 Score:  0.0\n",
            "C: 0.01\n",
            "Train Accuracy: 0.9360698608387037\n",
            "Train F1 Score:  0.1647345942647956\n",
            "Test Accuracy: 0.9346795601061813\n",
            "Test F1 Score:  0.13333333333333333\n",
            "C: 0.1\n",
            "Train Accuracy: 0.9688521527972355\n",
            "Train F1 Score:  0.7172530733361595\n",
            "Test Accuracy: 0.9549677664012135\n",
            "Test F1 Score:  0.5489078822412156\n",
            "C: 0.5\n",
            "Train Accuracy: 0.9992528252545064\n",
            "Train F1 Score:  0.9946524064171123\n",
            "Test Accuracy: 0.9635949943117179\n",
            "Test F1 Score:  0.6805324459234608\n",
            "C: 1\n",
            "Train Accuracy: 0.9997198094704399\n",
            "Train F1 Score:  0.998\n",
            "Test Accuracy: 0.9642586272279106\n",
            "Test F1 Score:  0.6957223567393059\n",
            "C: 10\n",
            "Train Accuracy: 0.9998132063136266\n",
            "Train F1 Score:  0.9986666666666667\n",
            "Test Accuracy: 0.9636897990140311\n",
            "Test F1 Score:  0.7033307513555384\n",
            "C: 100\n",
            "Train Accuracy: 0.9998132063136266\n",
            "Train F1 Score:  0.9986666666666667\n",
            "Test Accuracy: 0.9633105802047781\n",
            "Test F1 Score:  0.7006960556844547\n",
            "C: 1000\n",
            "Train Accuracy: 0.9998132063136266\n",
            "Train F1 Score:  0.9986666666666667\n",
            "Test Accuracy: 0.9636897990140311\n",
            "Test F1 Score:  0.7019455252918287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VRB-vTX_jgj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "73643766-cd54-472d-fb7c-300f1321a7d2"
      },
      "source": [
        "model = LinearSVC(penalty = 'l2', C = 10, dual = False, random_state = 0, max_iter = 1000)\n",
        "model.fit(data,y)\n",
        "y_pred_tr = model.predict(x_tr)\n",
        "y_pred_te = model.predict(x_tst)\n",
        "print(\"C: {}\".format(i))\n",
        "print('Train Accuracy:', accuracy_score(y_pred_tr, y_train))\n",
        "print(\"Train F1 Score: \", f1_score(y_pred_tr, y_train))\n",
        "print('Test Accuracy:', accuracy_score(y_pred_te, y_test))\n",
        "print(\"Test F1 Score: \", f1_score(y_pred_te, y_test))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C: 1000\nTrain Accuracy: 0.9997665078920333\nTrain F1 Score:  0.9983327775925308\nTest Accuracy: 0.9994311717861206\nTest F1 Score:  0.9959294436906377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCow5J26_quK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_cv=model.predict(x_cv)\n",
        "y_pred_cv=pd.DataFrame(y_pred_cv)\n",
        "y_pred_cv.to_csv('/content/drive/My Drive/Twitter Sentiment Analysis/test_predictions_linearSVC.csv', index=True)"
      ],
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XazhTThcWZJw",
        "colab_type": "text"
      },
      "source": [
        "**Using Decision Trees Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kn2-M2plWdYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "depth=[1,5,10,50,100, 500, 1000]\n",
        "samples_split=[5, 10, 20, 45, 75, 100, 135, 270, 500]\n",
        "train_f1_score=[]\n",
        "test_f1_score=[]\n",
        "for i in depth:\n",
        "    for j in samples_split:\n",
        "        clf=DecisionTreeClassifier(max_depth=i,min_samples_split=j,class_weight='balanced',random_state=0)\n",
        "        clf.fit(x_tr, y_train)\n",
        "        y_pred_tr = clf.predict(x_tr)\n",
        "        y_pred_te=clf.predict(x_tst)\n",
        "        train_f1 = f1_score(y_train, y_pred_tr)\n",
        "        test_f1 = f1_score(y_test, y_pred_te)\n",
        "        print(\"Depth: {} Samples_Split: {} Train F1 Score: {} Test F1 Score: {}\".format(i,j,train_f1,test_f1))\n",
        "        train_f1_score.append(train_f1)\n",
        "        test_f1_score.append(test_f1)\n",
        "    print('\\n')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Depth: 1 Samples_Split: 5 Train F1 Score: 0.17929810187081796 Test F1 Score: 0.18522632602054986\n",
            "Depth: 1 Samples_Split: 10 Train F1 Score: 0.17929810187081796 Test F1 Score: 0.18522632602054986\n",
            "Depth: 1 Samples_Split: 20 Train F1 Score: 0.17929810187081796 Test F1 Score: 0.18522632602054986\n",
            "Depth: 1 Samples_Split: 45 Train F1 Score: 0.17929810187081796 Test F1 Score: 0.18522632602054986\n",
            "Depth: 1 Samples_Split: 75 Train F1 Score: 0.17929810187081796 Test F1 Score: 0.18522632602054986\n",
            "Depth: 1 Samples_Split: 100 Train F1 Score: 0.17929810187081796 Test F1 Score: 0.18522632602054986\n",
            "Depth: 1 Samples_Split: 135 Train F1 Score: 0.17929810187081796 Test F1 Score: 0.18522632602054986\n",
            "Depth: 1 Samples_Split: 270 Train F1 Score: 0.17929810187081796 Test F1 Score: 0.18522632602054986\n",
            "Depth: 1 Samples_Split: 500 Train F1 Score: 0.17929810187081796 Test F1 Score: 0.18522632602054986\n",
            "\n",
            "\n",
            "Depth: 5 Samples_Split: 5 Train F1 Score: 0.32712915961646927 Test F1 Score: 0.32494279176201374\n",
            "Depth: 5 Samples_Split: 10 Train F1 Score: 0.32712915961646927 Test F1 Score: 0.32494279176201374\n",
            "Depth: 5 Samples_Split: 20 Train F1 Score: 0.32712915961646927 Test F1 Score: 0.32494279176201374\n",
            "Depth: 5 Samples_Split: 45 Train F1 Score: 0.3260869565217391 Test F1 Score: 0.32471482889733844\n",
            "Depth: 5 Samples_Split: 75 Train F1 Score: 0.3260258572231591 Test F1 Score: 0.3249619482496195\n",
            "Depth: 5 Samples_Split: 100 Train F1 Score: 0.3260258572231591 Test F1 Score: 0.3249619482496195\n",
            "Depth: 5 Samples_Split: 135 Train F1 Score: 0.3249906611878969 Test F1 Score: 0.32498101746393315\n",
            "Depth: 5 Samples_Split: 270 Train F1 Score: 0.3249906611878969 Test F1 Score: 0.32498101746393315\n",
            "Depth: 5 Samples_Split: 500 Train F1 Score: 0.3249906611878969 Test F1 Score: 0.32498101746393315\n",
            "\n",
            "\n",
            "Depth: 10 Samples_Split: 5 Train F1 Score: 0.37757800105764144 Test F1 Score: 0.3564781675017895\n",
            "Depth: 10 Samples_Split: 10 Train F1 Score: 0.37697993664202745 Test F1 Score: 0.35571428571428576\n",
            "Depth: 10 Samples_Split: 20 Train F1 Score: 0.3733658706641102 Test F1 Score: 0.35289957567185287\n",
            "Depth: 10 Samples_Split: 45 Train F1 Score: 0.3703959882413972 Test F1 Score: 0.35219683655536027\n",
            "Depth: 10 Samples_Split: 75 Train F1 Score: 0.36873816491650885 Test F1 Score: 0.35306479859894924\n",
            "Depth: 10 Samples_Split: 100 Train F1 Score: 0.366341713699333 Test F1 Score: 0.35253054101221637\n",
            "Depth: 10 Samples_Split: 135 Train F1 Score: 0.36527967257844474 Test F1 Score: 0.35273614499825723\n",
            "Depth: 10 Samples_Split: 270 Train F1 Score: 0.36416382252559726 Test F1 Score: 0.35273614499825723\n",
            "Depth: 10 Samples_Split: 500 Train F1 Score: 0.36416382252559726 Test F1 Score: 0.35273614499825723\n",
            "\n",
            "\n",
            "Depth: 50 Samples_Split: 5 Train F1 Score: 0.6879631995911065 Test F1 Score: 0.45051194539249145\n",
            "Depth: 50 Samples_Split: 10 Train F1 Score: 0.6751306945481703 Test F1 Score: 0.4412607449856733\n",
            "Depth: 50 Samples_Split: 20 Train F1 Score: 0.6267687311528648 Test F1 Score: 0.4263462394303516\n",
            "Depth: 50 Samples_Split: 45 Train F1 Score: 0.580297477904721 Test F1 Score: 0.40853913771452494\n",
            "Depth: 50 Samples_Split: 75 Train F1 Score: 0.5472779369627507 Test F1 Score: 0.4077669902912621\n",
            "Depth: 50 Samples_Split: 100 Train F1 Score: 0.5206755969714619 Test F1 Score: 0.39784532512504805\n",
            "Depth: 50 Samples_Split: 135 Train F1 Score: 0.5038833112331881 Test F1 Score: 0.39638962015795415\n",
            "Depth: 50 Samples_Split: 270 Train F1 Score: 0.4734109221128022 Test F1 Score: 0.380007089684509\n",
            "Depth: 50 Samples_Split: 500 Train F1 Score: 0.4711126961483595 Test F1 Score: 0.37933474876150036\n",
            "\n",
            "\n",
            "Depth: 100 Samples_Split: 5 Train F1 Score: 0.8135308918087784 Test F1 Score: 0.4591163026917217\n",
            "Depth: 100 Samples_Split: 10 Train F1 Score: 0.7799517555615116 Test F1 Score: 0.45208845208845205\n",
            "Depth: 100 Samples_Split: 20 Train F1 Score: 0.7087804878048781 Test F1 Score: 0.42972247090420773\n",
            "Depth: 100 Samples_Split: 45 Train F1 Score: 0.6507865168539326 Test F1 Score: 0.41301627033792243\n",
            "Depth: 100 Samples_Split: 75 Train F1 Score: 0.5985037406483791 Test F1 Score: 0.3942232630757221\n",
            "Depth: 100 Samples_Split: 100 Train F1 Score: 0.5725114701775383 Test F1 Score: 0.3941769316909295\n",
            "Depth: 100 Samples_Split: 135 Train F1 Score: 0.5572354211663066 Test F1 Score: 0.4\n",
            "Depth: 100 Samples_Split: 270 Train F1 Score: 0.5109146671477539 Test F1 Score: 0.3733609385783299\n",
            "Depth: 100 Samples_Split: 500 Train F1 Score: 0.5090778357001617 Test F1 Score: 0.3724137931034483\n",
            "\n",
            "\n",
            "Depth: 500 Samples_Split: 5 Train F1 Score: 0.9724830042084818 Test F1 Score: 0.4958677685950413\n",
            "Depth: 500 Samples_Split: 10 Train F1 Score: 0.9265885256014805 Test F1 Score: 0.4764957264957265\n",
            "Depth: 500 Samples_Split: 20 Train F1 Score: 0.8294166436273154 Test F1 Score: 0.4534373476352999\n",
            "Depth: 500 Samples_Split: 45 Train F1 Score: 0.7445219123505976 Test F1 Score: 0.4288224956063269\n",
            "Depth: 500 Samples_Split: 75 Train F1 Score: 0.6591362126245848 Test F1 Score: 0.40828157349896477\n",
            "Depth: 500 Samples_Split: 100 Train F1 Score: 0.6458787614478849 Test F1 Score: 0.40903590157321496\n",
            "Depth: 500 Samples_Split: 135 Train F1 Score: 0.6197718631178707 Test F1 Score: 0.40224809313528703\n",
            "Depth: 500 Samples_Split: 270 Train F1 Score: 0.5789577967109173 Test F1 Score: 0.3868586194167589\n",
            "Depth: 500 Samples_Split: 500 Train F1 Score: 0.5417515274949084 Test F1 Score: 0.38246730293389886\n",
            "\n",
            "\n",
            "Depth: 1000 Samples_Split: 5 Train F1 Score: 0.9724830042084818 Test F1 Score: 0.4958677685950413\n",
            "Depth: 1000 Samples_Split: 10 Train F1 Score: 0.9265885256014805 Test F1 Score: 0.4764957264957265\n",
            "Depth: 1000 Samples_Split: 20 Train F1 Score: 0.8294166436273154 Test F1 Score: 0.4534373476352999\n",
            "Depth: 1000 Samples_Split: 45 Train F1 Score: 0.7445219123505976 Test F1 Score: 0.4288224956063269\n",
            "Depth: 1000 Samples_Split: 75 Train F1 Score: 0.6591362126245848 Test F1 Score: 0.40828157349896477\n",
            "Depth: 1000 Samples_Split: 100 Train F1 Score: 0.6458787614478849 Test F1 Score: 0.40903590157321496\n",
            "Depth: 1000 Samples_Split: 135 Train F1 Score: 0.6197718631178707 Test F1 Score: 0.40224809313528703\n",
            "Depth: 1000 Samples_Split: 270 Train F1 Score: 0.5789577967109173 Test F1 Score: 0.3868586194167589\n",
            "Depth: 1000 Samples_Split: 500 Train F1 Score: 0.5417515274949084 Test F1 Score: 0.38246730293389886\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4lIRLbdWYPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "depth=[1,5,10,50,100, 500, 1000]\n",
        "samples_split=[5, 10, 20, 45, 75, 100, 135, 270, 500]\n",
        "train_f1_score=[]\n",
        "test_f1_score=[]\n",
        "for i in depth:\n",
        "    for j in samples_split:\n",
        "        clf = DecisionTreeClassifier(max_depth = i, min_samples_split = j, class_weight = 'balanced', random_state = 0)\n",
        "        clf.fit(x_tr, y_train)\n",
        "        y_pred_tr = clf.predict(x_tr)\n",
        "        y_pred_te = clf.predict(x_tst)\n",
        "        train_f1 = f1_score(y_train, y_pred_tr)\n",
        "        test_f1 = f1_score(y_test, y_pred_te)\n",
        "        print(\"Depth: {} Samples_Split: {} Train F1 Score: {} Test F1 Score: {}\".format(i,j,train_f1,test_f1))\n",
        "        train_f1_score.append(train_f1)\n",
        "        test_f1_score.append(test_f1)\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdP8KR24Wr-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf=DecisionTreeClassifier(max_depth = 1000, min_samples_split = 500, class_weight = 'balanced', random_state=0)\n",
        "clf.fit(x_tr, Yytrain)\n",
        "y_pred_tr = clf.predict(x_tr)\n",
        "y_pred_te = clf.predict(x_tst)\n",
        "y_pred_cv = clf.predict(_cv)\n",
        "train_score = f1_score(y_train, y_pred_tr)\n",
        "test_score = f1_score(y_test, y_pred_te)\n",
        "#cv_score=f1_score(Y_test,y_pred_cv)\n",
        "print(train_score, test_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RutHtHtTUhaa",
        "colab_type": "text"
      },
      "source": [
        "**Part 3: Using BOW features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA1gfx80U1pS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer() #in scikit-learn\n",
        "count_vect.fit(x_train)\n",
        "\n",
        "x_train_bow = count_vect.transform(x_train)\n",
        "x_test_bow = count_vect.transform(x_test)\n",
        "x_cv_bow = count_vect.transform(preprocessed_test)"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQ73Y1kGVwMA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dad77938-834b-4766-9ba4-adb0521fd227"
      },
      "source": [
        "x_tr = hstack((x_train_bow, ss_tr)).tocsr()\n",
        "x_cv = hstack((x_cv_bow, ss_cv)).tocsr()\n",
        "x_tst = hstack((x_test_bow, ss_test)).tocsr()\n",
        "print(x_tr.shape, x_tst.shape, x_cv.shape)"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(21414, 26942) (10548, 26942) (17197, 26942)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrC7CHemVkP0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "outputId": "50fbb79a-82a9-4ea0-e8b1-4033975a6c54"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import roc_auc_score,f1_score,accuracy_score\n",
        "C=[0.001,0.01,0.1,0.5,1,10,100,1000]\n",
        "for i in C:\n",
        "  model = LinearSVC(penalty = 'l2', C = i, dual = False, random_state = 0, max_iter = 1000)\n",
        "  model.fit(x_train_bow, y_train)\n",
        "  y_pred_tr = model.predict(x_train_bow)\n",
        "  y_pred_te = model.predict(x_test_bow)\n",
        "  print(\"C: {}\".format(i))\n",
        "  print('Train Accuracy:', accuracy_score(y_pred_tr, y_train))\n",
        "  print(\"Train F1 Score: \", f1_score(y_pred_tr, y_train))\n",
        "  print('Test Accuracy:', accuracy_score(y_pred_te, y_test))\n",
        "  print(\"Test F1 Score: \", f1_score(y_pred_te, y_test))"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C: 0.001\n",
            "Train Accuracy: 0.9300457644531614\n",
            "Train F1 Score:  0.007947019867549669\n",
            "Test Accuracy: 0.9298445202882063\n",
            "Test F1 Score:  0.0\n",
            "C: 0.01\n",
            "Train Accuracy: 0.9516671336508826\n",
            "Train F1 Score:  0.4868616757560733\n",
            "Test Accuracy: 0.94871065604854\n",
            "Test F1 Score:  0.44169246646026833\n",
            "C: 0.1\n",
            "Train Accuracy: 0.9860371719435883\n",
            "Train F1 Score:  0.8899521531100479\n",
            "Test Accuracy: 0.9635949943117179\n",
            "Test F1 Score:  0.6740237691001698\n",
            "C: 0.5\n",
            "Train Accuracy: 0.9977117773419258\n",
            "Train F1 Score:  0.9834515366430261\n",
            "Test Accuracy: 0.9654910883579826\n",
            "Test F1 Score:  0.7187017001545596\n",
            "C: 1\n",
            "Train Accuracy: 0.9987858410385729\n",
            "Train F1 Score:  0.9912810194500337\n",
            "Test Accuracy: 0.962836556693212\n",
            "Test F1 Score:  0.7048192771084338\n",
            "C: 10\n",
            "Train Accuracy: 0.9994396189408798\n",
            "Train F1 Score:  0.9959946595460614\n",
            "Test Accuracy: 0.9534508911642018\n",
            "Test F1 Score:  0.6587908269631689\n",
            "C: 100\n",
            "Train Accuracy: 0.9995797142056598\n",
            "Train F1 Score:  0.996996996996997\n",
            "Test Accuracy: 0.949089874857793\n",
            "Test F1 Score:  0.6369168356997973\n",
            "C: 1000\n",
            "Train Accuracy: 0.9996264126272532\n",
            "Train F1 Score:  0.9973297730307076\n",
            "Test Accuracy: 0.947478194918468\n",
            "Test F1 Score:  0.6276881720430109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCR_4Xh3CqbZ",
        "colab_type": "text"
      },
      "source": [
        "**Part 3: Using Tfidf Weighted Word2Vec features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WpRCHs2G5Vu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "with open('/content/drive/My Drive/glove_vectors', 'rb') as f:\n",
        "    model = pickle.load(f)\n",
        "    glove_words =  set(model.keys())"
      ],
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8fycA84JhBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dictionary = dict(zip(vectorizer_unigram.get_feature_names(), list(vectorizer_unigram.idf_)))"
      ],
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaEuG4L3H6W_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6924e5f2-5a52-4d99-d5c5-7dd30f17de07"
      },
      "source": [
        "from tqdm import tqdm\n",
        "tfidf_w2v_vectors_tr = []; # the avg-w2v for each sentence/review is stored in this list\n",
        "for sentence in tqdm(X_train): # for each review/sentence\n",
        "    vector = np.zeros(300) # as word vectors are of zero length\n",
        "    tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n",
        "    for word in sentence.split(): # for each word in a review/sentence\n",
        "        if (word in glove_words) and (word in tfidf_words):\n",
        "            vec = model[word] # getting the vector for each word\n",
        "            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n",
        "            tf_idf = dictionary[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n",
        "            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n",
        "            tf_idf_weight += tf_idf\n",
        "    if tf_idf_weight != 0:\n",
        "        vector /= tf_idf_weight\n",
        "    tfidf_w2v_vectors_tr.append(vector)"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 21414/21414 [00:33<00:00, 634.78it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNTB8yCmKq27",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f295acef-181c-47c4-cc62-e2aef236967d"
      },
      "source": [
        "tfidf_w2v_vectors_te = []; # the avg-w2v for each sentence/review is stored in this list\n",
        "for sentence in tqdm(X_test): # for each review/sentence\n",
        "    vector = np.zeros(300) # as word vectors are of zero length\n",
        "    tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n",
        "    for word in sentence.split(): # for each word in a review/sentence\n",
        "        if (word in glove_words) and (word in tfidf_words):\n",
        "            vec = model[word] # getting the vector for each word\n",
        "            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n",
        "            tf_idf = dictionary[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n",
        "            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n",
        "            tf_idf_weight += tf_idf\n",
        "    if tf_idf_weight != 0:\n",
        "        vector /= tf_idf_weight\n",
        "    tfidf_w2v_vectors_te.append(vector)"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10548/10548 [00:17<00:00, 610.55it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJe0NiimH-u5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7bb9a9fe-5cb0-481d-a2db-1e39e2dbb278"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "tfidf_w2v_vectors_cv = []; # the avg-w2v for each sentence/review is stored in this list\n",
        "    \n",
        "for sentence in tqdm(preprocessed_test): # for each review/sentence\n",
        "    vector = np.zeros(300) # as word vectors are of zero length\n",
        "    tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n",
        "    for word in sentence.split(): # for each word in a review/sentence\n",
        "        if (word in glove_words) and (word in tfidf_words):\n",
        "            vec = model[word] # getting the vector for each word\n",
        "            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n",
        "            tf_idf = dictionary[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n",
        "            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n",
        "            tf_idf_weight += tf_idf\n",
        "    if tf_idf_weight != 0:\n",
        "        vector /= tf_idf_weight\n",
        "    tfidf_w2v_vectors_cv.append(vector)"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17197/17197 [00:28<00:00, 612.95it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDVExBuMJ9QC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5efab593-135b-4427-91b6-d40f2413d7a3"
      },
      "source": [
        "tfidf_w2v_vectors_tr=np.array(tfidf_w2v_vectors_tr)\n",
        "tfidf_w2v_vectors_te=np.array(tfidf_w2v_vectors_te)\n",
        "tfidf_w2v_vectors_cv=np.array(tfidf_w2v_vectors_cv)\n",
        "print(tfidf_w2v_vectors_tr.shape,tfidf_w2v_vectors_te.shape)"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(21414, 300) (10548, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbCnxdHJNlpW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e80ac34b-ae78-4f75-aea8-1a0eef803382"
      },
      "source": [
        "tfidf_w2v_vectors_tr.shape,len(ss_tr[0])"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((21414, 300), 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbAbUoLFKQ39",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "outputId": "10695b28-c47d-4187-8d0d-44211088567d"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import roc_auc_score,f1_score,accuracy_score\n",
        "C=[0.001,0.01,0.1,0.5,1,10,100,1000]\n",
        "for i in C:\n",
        "  model = LinearSVC(penalty = 'l2', C = i, dual = False, random_state = 0, max_iter = 1000)\n",
        "  model.fit(tfidf_w2v_vectors_tr,Y_train)\n",
        "  y_pred_tr = model.predict(tfidf_w2v_vectors_tr)\n",
        "  y_pred_te = model.predict(tfidf_w2v_vectors_te)\n",
        "  print(\"C: {}\".format(i))\n",
        "  print('Train Accuracy:', accuracy_score(y_pred_tr, y_train))\n",
        "  print(\"Train F1 Score: \", f1_score(y_pred_tr, y_train))\n",
        "  print('Test Accuracy:', accuracy_score(y_pred_te, y_test))\n",
        "  print(\"Test F1 Score: \", f1_score(y_pred_te, y_test))"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C: 0.001\n",
            "Train Accuracy: 0.9374241150649109\n",
            "Train F1 Score:  0.24379232505643342\n",
            "Test Accuracy: 0.9373340917709518\n",
            "Test F1 Score:  0.2547914317925592\n",
            "C: 0.01\n",
            "Train Accuracy: 0.9425609414401793\n",
            "Train F1 Score:  0.39048562933597625\n",
            "Test Accuracy: 0.9416003033750474\n",
            "Test F1 Score:  0.39130434782608686\n",
            "C: 0.1\n",
            "Train Accuracy: 0.9448958625198468\n",
            "Train F1 Score:  0.44652908067542213\n",
            "Test Accuracy: 0.9429275692074327\n",
            "Test F1 Score:  0.4331450094161958\n",
            "C: 0.5\n",
            "Train Accuracy: 0.9453628467357803\n",
            "Train F1 Score:  0.4563197026022304\n",
            "Test Accuracy: 0.943022373909746\n",
            "Test F1 Score:  0.43779232927970063\n",
            "C: 1\n",
            "Train Accuracy: 0.9454095451573736\n",
            "Train F1 Score:  0.45653184565318455\n",
            "Test Accuracy: 0.9428327645051194\n",
            "Test F1 Score:  0.4369747899159664\n",
            "C: 10\n",
            "Train Accuracy: 0.9453628467357803\n",
            "Train F1 Score:  0.4568245125348189\n",
            "Test Accuracy: 0.9427379598028062\n",
            "Test F1 Score:  0.43656716417910446\n",
            "C: 100\n",
            "Train Accuracy: 0.9453628467357803\n",
            "Train F1 Score:  0.4568245125348189\n",
            "Test Accuracy: 0.9427379598028062\n",
            "Test F1 Score:  0.43656716417910446\n",
            "C: 1000\n",
            "Train Accuracy: 0.9453628467357803\n",
            "Train F1 Score:  0.4568245125348189\n",
            "Test Accuracy: 0.9427379598028062\n",
            "Test F1 Score:  0.43656716417910446\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}