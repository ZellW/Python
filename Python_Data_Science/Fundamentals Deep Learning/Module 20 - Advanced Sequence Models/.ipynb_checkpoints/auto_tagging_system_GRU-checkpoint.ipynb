{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135277,
     "status": "ok",
     "timestamp": 1584605679016,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "L_axH7-PrUFT",
    "outputId": "a15460e1-24b1-41a3-9ec8-d135f99bd50d"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WtPKZPpFxD6g"
   },
   "source": [
    "# Objective\n",
    "\n",
    "Build a model to automatically predict tags for a given a StackExchange question by using the text of the question.\n",
    "![alt text](https://cdn.sstatic.net/Sites/stackoverflow/company/img/logos/se/se-logo.svg?v=d29f0785ebb7)\n",
    "\n",
    "__Dataset Specs__: Over 85,000 questions\n",
    "\n",
    "[Download Link](https://www.kaggle.com/stackoverflow/statsquestions#Questions.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HfaOlnHpy-Va"
   },
   "source": [
    "# Steps to Follow\n",
    "\n",
    "\n",
    "\n",
    "1. Load Data and Import Libraries\n",
    "2. Text Cleaning\n",
    "3. Merge Tags with Questions\n",
    "4. Dataset Prepartion\n",
    "5. Text Representation\n",
    "6. Model Building\n",
    "    1. Define Model Architecture\n",
    "    2. Train the Model\n",
    "7. Model Predictions\n",
    "8. Model Evaluation\n",
    "9. Inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uVFrjcv4H_pa"
   },
   "source": [
    "# Load Data and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12001,
     "status": "ok",
     "timestamp": 1584605694201,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "m_fivLMXrYDM",
    "outputId": "3a327942-85b4-405a-997b-47d86011d932"
   },
   "outputs": [],
   "source": [
    "# !unzip '/content/drive/My Drive/Course_Notes/Applied DL - Sequence Models/Project 1: Automatic Tagging System/statsquestions.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gitKl0VQbmBN"
   },
   "outputs": [],
   "source": [
    "#string matching\n",
    "import re \n",
    "\n",
    "#reading files\n",
    "import pandas as pd\n",
    "\n",
    "#handling html data\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#visualization\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qLXg0bRfclmw"
   },
   "outputs": [],
   "source": [
    "# load the stackoverflow questions dataset\n",
    "questions_df = pd.read_csv('../../../../../LargeData/Analytics_Vidhya/StackOverflow_tagging/Questions.csv',encoding='latin-1')\n",
    "\n",
    "# load the tags dataset\n",
    "tags_df = pd.read_csv('../../../../../LargeData/Analytics_Vidhya/StackOverflow_tagging/Tags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8891,
     "status": "ok",
     "timestamp": 1584605696318,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "8pxycLMRKvO4",
    "outputId": "ccea4c8f-96a7-45bc-e691-e67ae8a08e12"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Score</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2010-07-19T19:14:44Z</td>\n",
       "      <td>272</td>\n",
       "      <td>The Two Cultures: statistics vs. machine learning?</td>\n",
       "      <td>&lt;p&gt;Last year, I read a blog post from &lt;a href=\"http://anyall.org/\"&gt;Brendan O'Connor&lt;/a&gt; entitled &lt;a href=\"http://anyall.org/blog/2008/12/statistics-vs-machine-learning-fight/\"&gt;\"Statistics vs. Mach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2010-07-19T19:24:36Z</td>\n",
       "      <td>4</td>\n",
       "      <td>Forecasting demographic census</td>\n",
       "      <td>&lt;p&gt;What are some of the ways to forecast demographic census with some validation and calibration techniques?&lt;/p&gt;\\n\\n&lt;p&gt;Some of the concerns:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Census blocks vary in sizes as rural\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>66.0</td>\n",
       "      <td>2010-07-19T19:25:39Z</td>\n",
       "      <td>208</td>\n",
       "      <td>Bayesian and frequentist reasoning in plain English</td>\n",
       "      <td>&lt;p&gt;How would you describe in plain English the characteristics that distinguish Bayesian from Frequentist reasoning?&lt;/p&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2010-07-19T19:28:44Z</td>\n",
       "      <td>138</td>\n",
       "      <td>What is the meaning of p values and t values in statistical tests?</td>\n",
       "      <td>&lt;p&gt;After taking a statistics course and then trying to help fellow students, I noticed one subject that inspires much head-desk banging is interpreting the results of statistical hypothesis tests....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2010-07-19T19:31:47Z</td>\n",
       "      <td>58</td>\n",
       "      <td>Examples for teaching: Correlation does not mean causation</td>\n",
       "      <td>&lt;p&gt;There is an old saying: \"Correlation does not mean causation\". When I teach, I tend to use the following standard examples to illustrate this point:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;number of storks and birth ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  OwnerUserId          CreationDate  Score  \\\n",
       "0   6          5.0  2010-07-19T19:14:44Z    272   \n",
       "1  21         59.0  2010-07-19T19:24:36Z      4   \n",
       "2  22         66.0  2010-07-19T19:25:39Z    208   \n",
       "3  31         13.0  2010-07-19T19:28:44Z    138   \n",
       "4  36          8.0  2010-07-19T19:31:47Z     58   \n",
       "\n",
       "                                                                Title  \\\n",
       "0                  The Two Cultures: statistics vs. machine learning?   \n",
       "1                                      Forecasting demographic census   \n",
       "2                 Bayesian and frequentist reasoning in plain English   \n",
       "3  What is the meaning of p values and t values in statistical tests?   \n",
       "4          Examples for teaching: Correlation does not mean causation   \n",
       "\n",
       "                                                                                                                                                                                                      Body  \n",
       "0  <p>Last year, I read a blog post from <a href=\"http://anyall.org/\">Brendan O'Connor</a> entitled <a href=\"http://anyall.org/blog/2008/12/statistics-vs-machine-learning-fight/\">\"Statistics vs. Mach...  \n",
       "1  <p>What are some of the ways to forecast demographic census with some validation and calibration techniques?</p>\\n\\n<p>Some of the concerns:</p>\\n\\n<ul>\\n<li>Census blocks vary in sizes as rural\\n...  \n",
       "2                                                                               <p>How would you describe in plain English the characteristics that distinguish Bayesian from Frequentist reasoning?</p>\\n  \n",
       "3  <p>After taking a statistics course and then trying to help fellow students, I noticed one subject that inspires much head-desk banging is interpreting the results of statistical hypothesis tests....  \n",
       "4  <p>There is an old saying: \"Correlation does not mean causation\". When I teach, I tend to use the following standard examples to illustrate this point:</p>\\n\\n<ol>\\n<li>number of storks and birth ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print first 5 rows\n",
    "questions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "siBpKFe3IZNw"
   },
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hmeuzAF9Ic1d"
   },
   "source": [
    "Let's define a function to clean the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ADh9l-RWNSDU"
   },
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "\n",
    "  text = BeautifulSoup(text).get_text()\n",
    "  \n",
    "  # fetch alphabetic characters\n",
    "  text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "\n",
    "  # convert text to lower case\n",
    "  text = text.lower()\n",
    "\n",
    "  # split text into tokens to remove whitespaces\n",
    "  tokens = text.split()\n",
    "\n",
    "  return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o98nUFycNSB-"
   },
   "outputs": [],
   "source": [
    "# call preprocessing function\n",
    "questions_df['cleaned_text'] = questions_df['Body'].apply(cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 37085,
     "status": "ok",
     "timestamp": 1584605728855,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "umvS_4ZQNR6r",
    "outputId": "ce197470-d623-41ee-e1c9-33ac9d062a20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<p>What are some of the ways to forecast demographic census with some validation and calibration techniques?</p>\\n\\n<p>Some of the concerns:</p>\\n\\n<ul>\\n<li>Census blocks vary in sizes as rural\\nareas are a lot larger than condensed\\nurban areas. Is there a need to account for the area size difference?</li>\\n<li>if let's say I have census data\\ndating back to 4 - 5 census periods,\\nhow far can i forecast it into the\\nfuture?</li>\\n<li>if some of the census zone change\\nlightly in boundaries, how can i\\naccount for that change?</li>\\n<li>What are the methods to validate\\ncensus forecasts? for example, if i\\nhave data for existing 5 census\\nperiods, should I model the first 3\\nand test it on the latter two? or is\\nthere another way?</li>\\n<li>what's the state of practice in\\nforecasting census data, and what are\\nsome of the state of the art methods?</li>\\n</ul>\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_df['Body'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36618,
     "status": "ok",
     "timestamp": 1584605728858,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "FoEZ0aQ7KvN2",
    "outputId": "ad4add20-15c4-4b33-fbeb-b9f06661e3d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what are some of the ways to forecast demographic census with some validation and calibration techniques some of the concerns census blocks vary in sizes as rural areas are a lot larger than condensed urban areas is there a need to account for the area size difference if let s say i have census data dating back to census periods how far can i forecast it into the future if some of the census zone change lightly in boundaries how can i account for that change what are the methods to validate census forecasts for example if i have data for existing census periods should i model the first and test it on the latter two or is there another way what s the state of practice in forecasting census data and what are some of the state of the art methods'"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_df['cleaned_text'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dTPISO70LOM4"
   },
   "source": [
    "# Merge Tags with Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jTXwGw2xKhWk"
   },
   "source": [
    "Let's now explore the tags data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 27632,
     "status": "ok",
     "timestamp": 1584605249595,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "4p9JAFA9tTxO",
    "outputId": "43b19784-00d6-4478-ad68-9f7c5388737b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>bayesian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>prior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>elicitation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>distributions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>normality</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id            Tag\n",
       "0   1       bayesian\n",
       "1   1          prior\n",
       "2   1    elicitation\n",
       "3   2  distributions\n",
       "4   2      normality"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30895,
     "status": "ok",
     "timestamp": 1584605728859,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "rVGubRcWe01J",
    "outputId": "b70a9d30-1a97-442d-9763-fed771050598"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1315"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count of unique tags\n",
    "len(tags_df['Tag'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 24920,
     "status": "ok",
     "timestamp": 1584605249599,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "ZqW5nBknesV-",
    "outputId": "f421055d-3831-4aff-f157-7be8913e6537"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r                   13236\n",
       "regression          10959\n",
       "machine-learning     6089\n",
       "time-series          5559\n",
       "probability          4217\n",
       "                    ...  \n",
       "fmincon                 1\n",
       "sympy                   1\n",
       "segmented               1\n",
       "hawkes                  1\n",
       "pit                     1\n",
       "Name: Tag, Length: 1315, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_df['Tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3SEjB4OwiJ0b"
   },
   "outputs": [],
   "source": [
    "# remove \"-\" from the tags\n",
    "tags_df['Tag']= tags_df['Tag'].apply(lambda x:re.sub(\"-\",\" \",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29041,
     "status": "ok",
     "timestamp": 1584605731120,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "MhPvTj6ytcW0",
    "outputId": "6fd5ea5a-dd12-4766-d581-c093ecccbeeb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[bayesian, prior, elicitation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[distributions, normality]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[software, open source]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[distributions, statistical significance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>[machine learning]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                       tags\n",
       "0   1             [bayesian, prior, elicitation]\n",
       "1   2                 [distributions, normality]\n",
       "2   3                    [software, open source]\n",
       "3   4  [distributions, statistical significance]\n",
       "4   6                         [machine learning]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group tags Id wise\n",
    "tags_df = tags_df.groupby('Id').apply(lambda x:x['Tag'].values).reset_index(name='tags')\n",
    "tags_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zCnEKm1ouTnm"
   },
   "outputs": [],
   "source": [
    "# merge tags and questions\n",
    "df = pd.merge(questions_df,tags_df,how='inner',on='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23528,
     "status": "ok",
     "timestamp": 1584605252581,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "fn37BQ5TusSt",
    "outputId": "ad58d0fa-760d-4823-9f3a-626f5a815560"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Body</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>&lt;p&gt;Last year, I read a blog post from &lt;a href=\"http://anyall.org/\"&gt;Brendan O'Connor&lt;/a&gt; entitled &lt;a href=\"http://anyall.org/blog/2008/12/statistics-vs-machine-learning-fight/\"&gt;\"Statistics vs. Mach...</td>\n",
       "      <td>last year i read a blog post from brendan o connor entitled statistics vs machine learning fight that discussed some of the differences between the two fields andrew gelman responded favorably to ...</td>\n",
       "      <td>[machine learning]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>&lt;p&gt;What are some of the ways to forecast demographic census with some validation and calibration techniques?&lt;/p&gt;\\n\\n&lt;p&gt;Some of the concerns:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Census blocks vary in sizes as rural\\n...</td>\n",
       "      <td>what are some of the ways to forecast demographic census with some validation and calibration techniques some of the concerns census blocks vary in sizes as rural areas are a lot larger than conde...</td>\n",
       "      <td>[forecasting, population, census]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>&lt;p&gt;How would you describe in plain English the characteristics that distinguish Bayesian from Frequentist reasoning?&lt;/p&gt;\\n</td>\n",
       "      <td>how would you describe in plain english the characteristics that distinguish bayesian from frequentist reasoning</td>\n",
       "      <td>[bayesian, frequentist]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>&lt;p&gt;After taking a statistics course and then trying to help fellow students, I noticed one subject that inspires much head-desk banging is interpreting the results of statistical hypothesis tests....</td>\n",
       "      <td>after taking a statistics course and then trying to help fellow students i noticed one subject that inspires much head desk banging is interpreting the results of statistical hypothesis tests it s...</td>\n",
       "      <td>[hypothesis testing, t test, p value, interpretation, intuition]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36</td>\n",
       "      <td>&lt;p&gt;There is an old saying: \"Correlation does not mean causation\". When I teach, I tend to use the following standard examples to illustrate this point:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;number of storks and birth ...</td>\n",
       "      <td>there is an old saying correlation does not mean causation when i teach i tend to use the following standard examples to illustrate this point number of storks and birth rate in denmark number of ...</td>\n",
       "      <td>[correlation, teaching]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  \\\n",
       "0   6   \n",
       "1  21   \n",
       "2  22   \n",
       "3  31   \n",
       "4  36   \n",
       "\n",
       "                                                                                                                                                                                                      Body  \\\n",
       "0  <p>Last year, I read a blog post from <a href=\"http://anyall.org/\">Brendan O'Connor</a> entitled <a href=\"http://anyall.org/blog/2008/12/statistics-vs-machine-learning-fight/\">\"Statistics vs. Mach...   \n",
       "1  <p>What are some of the ways to forecast demographic census with some validation and calibration techniques?</p>\\n\\n<p>Some of the concerns:</p>\\n\\n<ul>\\n<li>Census blocks vary in sizes as rural\\n...   \n",
       "2                                                                               <p>How would you describe in plain English the characteristics that distinguish Bayesian from Frequentist reasoning?</p>\\n   \n",
       "3  <p>After taking a statistics course and then trying to help fellow students, I noticed one subject that inspires much head-desk banging is interpreting the results of statistical hypothesis tests....   \n",
       "4  <p>There is an old saying: \"Correlation does not mean causation\". When I teach, I tend to use the following standard examples to illustrate this point:</p>\\n\\n<ol>\\n<li>number of storks and birth ...   \n",
       "\n",
       "                                                                                                                                                                                              cleaned_text  \\\n",
       "0  last year i read a blog post from brendan o connor entitled statistics vs machine learning fight that discussed some of the differences between the two fields andrew gelman responded favorably to ...   \n",
       "1  what are some of the ways to forecast demographic census with some validation and calibration techniques some of the concerns census blocks vary in sizes as rural areas are a lot larger than conde...   \n",
       "2                                                                                         how would you describe in plain english the characteristics that distinguish bayesian from frequentist reasoning   \n",
       "3  after taking a statistics course and then trying to help fellow students i noticed one subject that inspires much head desk banging is interpreting the results of statistical hypothesis tests it s...   \n",
       "4  there is an old saying correlation does not mean causation when i teach i tend to use the following standard examples to illustrate this point number of storks and birth rate in denmark number of ...   \n",
       "\n",
       "                                                               tags  \n",
       "0                                                [machine learning]  \n",
       "1                                 [forecasting, population, census]  \n",
       "2                                           [bayesian, frequentist]  \n",
       "3  [hypothesis testing, t test, p value, interpretation, intuition]  \n",
       "4                                           [correlation, teaching]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['Id','Body','cleaned_text','tags']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22198,
     "status": "ok",
     "timestamp": 1584605252582,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "2cqkOjieMKBk",
    "outputId": "04999778-f969-4a14-ee45-ba458021f056"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85085, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YV6KXBw1MRyY"
   },
   "source": [
    "There are over 85,000 unique questions and over 1300 tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c_22ook_MiRv"
   },
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3uj_0l0jwL3f"
   },
   "outputs": [],
   "source": [
    "# check frequency of occurence of each tag\n",
    "freq= {}\n",
    "for i in df['tags']:\n",
    "  for j in i:\n",
    "    if j in freq.keys():\n",
    "      freq[j] = freq[j] + 1\n",
    "    else:\n",
    "      freq[j] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CUDiameNNhPh"
   },
   "source": [
    "Let's find out the most frequent tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WOzWyGfLzli9"
   },
   "outputs": [],
   "source": [
    "# sort the dictionary in descending order\n",
    "freq = dict(sorted(freq.items(), key=lambda x:x[1],reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20407,
     "status": "ok",
     "timestamp": 1584605731131,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "vKcagvyi0Wfk",
    "outputId": "667b1454-c1bf-4d3c-c5be-fbf9d03a874e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'r': 13236,\n",
       " 'regression': 10959,\n",
       " 'machine learning': 6089,\n",
       " 'time series': 5559,\n",
       " 'probability': 4217,\n",
       " 'hypothesis testing': 3869,\n",
       " 'self study': 3732,\n",
       " 'distributions': 3501,\n",
       " 'logistic': 3316,\n",
       " 'classification': 2881}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# freq.items() # too much data returned\n",
    "\n",
    "# Simple way to slice a dictionary\n",
    "import itertools\n",
    "\n",
    "dict(itertools.islice(freq.items(), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19032,
     "status": "ok",
     "timestamp": 1584605731133,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "1PQMi8WIv0_u",
    "outputId": "ebd18092-631e-4e19-9695-42ea4c18c884"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['r',\n",
       " 'regression',\n",
       " 'machine learning',\n",
       " 'time series',\n",
       " 'probability',\n",
       " 'hypothesis testing',\n",
       " 'self study',\n",
       " 'distributions',\n",
       " 'logistic',\n",
       " 'classification']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 most frequent tags\n",
    "common_tags = list(freq.keys())[:10]\n",
    "common_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "buPS2OrlN50F"
   },
   "source": [
    "We will use only those questions/queries that have the above 10 tags associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CVB3DKppym51"
   },
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "\n",
    "for i in range(len(df['tags'])):\n",
    "  \n",
    "  temp=[]\n",
    "  for j in df['tags'][i]:\n",
    "    if j in common_tags:\n",
    "      temp.append(j)\n",
    "\n",
    "  if(len(temp)>1):\n",
    "    x.append(df['cleaned_text'][i])\n",
    "    y.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17712,
     "status": "ok",
     "timestamp": 1584605732473,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "fYMxubGzzN7Q",
    "outputId": "ffe1f162-3593-4e27-defa-bd4eb3159962"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11106"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of questions left\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17044,
     "status": "ok",
     "timestamp": 1584605732474,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "hFmgSxWp00Gu",
    "outputId": "2542c276-a831-4376-b43a-f1ce5b367f1c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['r', 'time series'],\n",
       " ['regression', 'distributions'],\n",
       " ['distributions', 'probability', 'hypothesis testing'],\n",
       " ['hypothesis testing', 'self study'],\n",
       " ['r', 'regression', 'time series'],\n",
       " ['r', 'time series', 'self study'],\n",
       " ['probability', 'hypothesis testing'],\n",
       " ['r', 'regression'],\n",
       " ['r', 'regression'],\n",
       " ['regression', 'logistic']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XQuhftlBO0nd"
   },
   "source": [
    "Now we will find a suitable sequence length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C0okjkZUPEBJ"
   },
   "source": [
    "We will the input sequences to our model to the length of 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14416,
     "status": "ok",
     "timestamp": 1584605732476,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "pzCk5T-KR-W5",
    "outputId": "e967ca99-3d24-46b6-eb40-194005d1e656"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11106, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    " \n",
    "y = mlb.fit_transform(y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13901,
     "status": "ok",
     "timestamp": 1584605732478,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "eJxESqMJSEKZ",
    "outputId": "03a0fea9-ec2f-45cb-aa11-66e0fff6e8eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1323,
     "status": "ok",
     "timestamp": 1584605282228,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "YZHXV-GqSg3s",
    "outputId": "6bb6dfbc-e85a-4c0b-c6d7-c77eb77a228a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['classification', 'distributions', 'hypothesis testing',\n",
       "       'logistic', 'machine learning', 'probability', 'r', 'regression',\n",
       "       'self study', 'time series'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WkF4pDaJStjg"
   },
   "source": [
    "We can now split the dataset into training set and validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QPHeAiS9KvD9"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_tr,x_val,y_tr,y_val=train_test_split(x, y, test_size=0.2, random_state=0,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H4GaDNwtTmPh"
   },
   "source": [
    "# Text Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6799,
     "status": "ok",
     "timestamp": 1584605739290,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "TtrDalDiPXJM",
    "outputId": "51a872e3-7869-459d-d39e-67f8ee54ca8e"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "\n",
    "#prepare a tokenizer\n",
    "x_tokenizer = Tokenizer() \n",
    "\n",
    "#prepare vocabulary\n",
    "x_tokenizer.fit_on_texts(x_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6761,
     "status": "ok",
     "timestamp": 1584605739292,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "zl1vZKpwU2P2",
    "outputId": "1649f541-e256-4cfc-fca1-4434f1019463"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'i': 2,\n",
       " 'to': 3,\n",
       " 'a': 4,\n",
       " 'of': 5,\n",
       " 'is': 6,\n",
       " 'and': 7,\n",
       " 'in': 8,\n",
       " 'l': 9,\n",
       " 'x': 10,\n",
       " 'for': 11,\n",
       " 'that': 12,\n",
       " 'data': 13,\n",
       " 'this': 14,\n",
       " 't': 15,\n",
       " 'have': 16,\n",
       " 'y': 17,\n",
       " 'with': 18,\n",
       " 'model': 19,\n",
       " 'it': 20}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_tokenizer.word_index # returns too much data\n",
    "\n",
    "# List the first 20 items in the dictionary.  Note the numbers is the row, not count of occurence!\n",
    "dict(itertools.islice(x_tokenizer.word_index.items(), 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6729,
     "status": "ok",
     "timestamp": 1584605739294,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "Srz9RWKvVsm7",
    "outputId": "ddbc0aef-2b8b-4f9f-d9dd-a998aadd5b26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25315"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nQLwSve9VYqN"
   },
   "source": [
    "There are around 25,000 tokens in the training dataset. Let's see how many tokens appear at least 5 times in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3912,
     "status": "ok",
     "timestamp": 1584605739294,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "H-d_UjVmPjgo",
    "outputId": "52cebca5-ebb7-44f7-f0db-3c5ad38d5f75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12575\n"
     ]
    }
   ],
   "source": [
    "thresh = 3\n",
    "\n",
    "cnt=0\n",
    "for key,value in x_tokenizer.word_counts.items():\n",
    "  if value>=thresh:\n",
    "    cnt=cnt+1\n",
    "\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Eqly3dnVh-S"
   },
   "source": [
    "Over 12,000 tokens have appeared three times or more in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FZrJnr-dItPn"
   },
   "outputs": [],
   "source": [
    "# prepare the tokenizer again\n",
    "x_tokenizer = Tokenizer(num_words=cnt,oov_token='unk')\n",
    "\n",
    "#prepare vocabulary\n",
    "x_tokenizer.fit_on_texts(x_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJFfvLDJWZbb"
   },
   "source": [
    "Now that we have encoded every token to an integer, let's convert the text sequences to integer sequences. After that we will pad the integer sequences to the maximum sequence length, i.e., 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VpJvPYx5WR07"
   },
   "outputs": [],
   "source": [
    "#define threshold for maximum length of a setence\n",
    "max_len=100\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "x_tr_seq = x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val_seq = x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "#padding up with zero \n",
    "x_tr_seq = pad_sequences(x_tr_seq,  padding='post', maxlen=max_len)\n",
    "x_val_seq = pad_sequences(x_val_seq, padding='post', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6TkeQpLgXled"
   },
   "source": [
    "Since we are padding the sequences with zeros, we must increment the vocabulary size by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1172,
     "status": "ok",
     "timestamp": 1584605750699,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "Q4na29hBItes",
    "outputId": "de52b4c7-4f7c-4de2-934f-cb8b139eb665"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12576"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no. of unique words\n",
    "x_voc_size = x_tokenizer.num_words + 1\n",
    "x_voc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1361,
     "status": "ok",
     "timestamp": 1584605752778,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "A9jdhRc_O12J",
    "outputId": "0549ef57-f53d-4d7e-c1aa-b6bf2b6b2013"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1953, 5711,  416, 2023,    1,  226, 1747, 3740,  609,   43,  181,\n",
       "       1953,  372,   19,  100,  416,    9, 1747, 3839,  238,   27,   27,\n",
       "         27,   27,   27,   70,    6, 6919,    8, 1163,   70,    6,   43,\n",
       "         43, 1802, 1802, 1802,   36,   36,   36,   36, 4308, 5410,    4,\n",
       "        124,  592,  107,   22,    2, 1747, 4065,   27,   10, 1309,   10,\n",
       "       6415,   10,  190,   10,  416,   10,   27,   10, 1309,   10, 6415,\n",
       "         10,  190,   10,  416,   10,  456,  139,   15,    7,    2, 4610,\n",
       "        164,   27,   10, 1309,   10, 6415,   10,  190,   10,  416,   10,\n",
       "         27,   76,   27, 1309,   76,   27, 6415,   76,   27,  190,   76,\n",
       "         27])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr_seq[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BCtPnSrsscN1"
   },
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RrRgqQ4M8OZu"
   },
   "outputs": [],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxE6IK3Uic-d"
   },
   "source": [
    "### Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vHtamwcMkVcr"
   },
   "outputs": [],
   "source": [
    "#sequential model\n",
    "model = Sequential()\n",
    "\n",
    "#embedding layer\n",
    "model.add(Embedding(x_voc_size, 50, trainable = True, input_shape=(max_len,),mask_zero=True))\n",
    "\n",
    "#lstm \n",
    "model.add(GRU(128)) # This is the ONLY difference!\n",
    "\n",
    "#dense layer\n",
    "model.add(Dense(128,activation='relu')) \n",
    "\n",
    "#output layer\n",
    "model.add(Dense(10,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1551,
     "status": "ok",
     "timestamp": 1584605777275,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "6K14UgT--fCk",
    "outputId": "fcf5b090-3e0b-42a2-e465-ce3531ac1561"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 50)           628800    \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 128)               69120     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 715,722\n",
      "Trainable params: 715,722\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tzRoTFVIItjK"
   },
   "outputs": [],
   "source": [
    "#define optimizer and loss\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy')\n",
    "\n",
    "#checkpoint to save best model during training\n",
    "mc = ModelCheckpoint(\"../../../../../LargeData/Analytics_Vidhya/weights.best_GRU.hdf5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "80gtKbElii0e"
   },
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 138823,
     "status": "ok",
     "timestamp": 1584605934581,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "XH8ggzcdkzkL",
    "outputId": "8104a81a-7e20-4dee-f30f-7b34c541da44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5237\n",
      "Epoch 00001: val_loss improved from inf to 0.48053, saving model to ../../../../../LargeData/Analytics_Vidhya\\weights.best_GRU.hdf5\n",
      "70/70 [==============================] - 12s 179ms/step - loss: 0.5237 - val_loss: 0.4805\n",
      "Epoch 2/10\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4751\n",
      "Epoch 00002: val_loss improved from 0.48053 to 0.47276, saving model to ../../../../../LargeData/Analytics_Vidhya\\weights.best_GRU.hdf5\n",
      "70/70 [==============================] - 10s 138ms/step - loss: 0.4751 - val_loss: 0.4728\n",
      "Epoch 3/10\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4332\n",
      "Epoch 00003: val_loss improved from 0.47276 to 0.41614, saving model to ../../../../../LargeData/Analytics_Vidhya\\weights.best_GRU.hdf5\n",
      "70/70 [==============================] - 10s 148ms/step - loss: 0.4332 - val_loss: 0.4161\n",
      "Epoch 4/10\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.3904\n",
      "Epoch 00004: val_loss improved from 0.41614 to 0.40423, saving model to ../../../../../LargeData/Analytics_Vidhya\\weights.best_GRU.hdf5\n",
      "70/70 [==============================] - 12s 166ms/step - loss: 0.3904 - val_loss: 0.4042\n",
      "Epoch 5/10\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.3547\n",
      "Epoch 00005: val_loss improved from 0.40423 to 0.37029, saving model to ../../../../../LargeData/Analytics_Vidhya\\weights.best_GRU.hdf5\n",
      "70/70 [==============================] - 14s 195ms/step - loss: 0.3547 - val_loss: 0.3703\n",
      "Epoch 6/10\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.3140\n",
      "Epoch 00006: val_loss improved from 0.37029 to 0.35565, saving model to ../../../../../LargeData/Analytics_Vidhya\\weights.best_GRU.hdf5\n",
      "70/70 [==============================] - 13s 181ms/step - loss: 0.3140 - val_loss: 0.3556\n",
      "Epoch 7/10\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2758\n",
      "Epoch 00007: val_loss improved from 0.35565 to 0.34452, saving model to ../../../../../LargeData/Analytics_Vidhya\\weights.best_GRU.hdf5\n",
      "70/70 [==============================] - 16s 226ms/step - loss: 0.2758 - val_loss: 0.3445\n",
      "Epoch 8/10\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2413- ETA: 3\n",
      "Epoch 00008: val_loss did not improve from 0.34452\n",
      "70/70 [==============================] - 16s 224ms/step - loss: 0.2413 - val_loss: 0.3462\n",
      "Epoch 9/10\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2145\n",
      "Epoch 00009: val_loss did not improve from 0.34452\n",
      "70/70 [==============================] - 16s 231ms/step - loss: 0.2145 - val_loss: 0.3506\n",
      "Epoch 10/10\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1902\n",
      "Epoch 00010: val_loss did not improve from 0.34452\n",
      "70/70 [==============================] - 17s 250ms/step - loss: 0.1902 - val_loss: 0.3661\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20f48cc46d0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model \n",
    "model.fit(x_tr_seq, y_tr, batch_size=128, epochs=10, verbose=1, validation_data=(x_val_seq, y_val), callbacks=[mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JDzen8xvioUd"
   },
   "source": [
    "# Model Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xL8qz8zvDvpH"
   },
   "outputs": [],
   "source": [
    "# load weights into new model\n",
    "model.load_weights(\"../../../../../LargeData/Analytics_Vidhya/weights.best_GRU.hdf5\")\n",
    "\n",
    "#predict probabilities\n",
    "pred_prob = model.predict(x_val_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1409,
     "status": "ok",
     "timestamp": 1584605959983,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "AUF5H7bIlRLr",
    "outputId": "2327dfa6-a6f5-4da8-ceea-32186c65fc62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00528246, 0.00497261, 0.05560711, 0.04085082, 0.03063798,\n",
       "       0.00221953, 0.84268725, 0.7436102 , 0.02581331, 0.36820385],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_prob[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ph2xAWbFjLv5"
   },
   "source": [
    "The predictions are in terms of probabilities for each of the 10 tags. Hence we need to have a threshold value to convert these probabilities to 0 or 1.\n",
    "\n",
    "Let's specify a set of candidate threshold values. We will select the threshold value that performs the best for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1441,
     "status": "ok",
     "timestamp": 1584605963968,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "5hYDCTKXguMF",
    "outputId": "96a4a68b-e42d-455d-cd42-97915e9877af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ,\n",
       "       0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21,\n",
       "       0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32,\n",
       "       0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43,\n",
       "       0.44, 0.45, 0.46, 0.47, 0.48, 0.49])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#define candidate threshold values\n",
    "threshold  = np.arange(0,0.5,0.01)\n",
    "threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NA6wMIewkICl"
   },
   "source": [
    "Let's define a function that takes a threshold value and uses it to convert probabilities into 1 or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aay56TvDGPoX"
   },
   "outputs": [],
   "source": [
    "# convert probabilities into classes or tags based on a threshold value\n",
    "def classify(pred_prob,thresh):\n",
    "  y_pred_seq = []\n",
    "\n",
    "  for i in pred_prob:\n",
    "    temp=[]\n",
    "    for j in i:\n",
    "      if j>=thresh:\n",
    "        temp.append(1)\n",
    "      else:\n",
    "        temp.append(0)\n",
    "    y_pred_seq.append(temp)\n",
    "\n",
    "  return y_pred_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s0auJJmNDtv9"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "score=[]\n",
    "\n",
    "#convert to 1 array\n",
    "y_true = np.array(y_val).ravel() \n",
    "\n",
    "for thresh in threshold:\n",
    "    \n",
    "    #classes for each threshold\n",
    "    y_pred_seq = classify(pred_prob,thresh) \n",
    "\n",
    "    #convert to 1d array\n",
    "    y_pred = np.array(y_pred_seq).ravel()\n",
    "\n",
    "    score.append(metrics.f1_score(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3381,
     "status": "ok",
     "timestamp": 1584605971070,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "jrA8nJIGVBsl",
    "outputId": "022dfc2a-2a5c-41f2-e2cf-56ec3ffbcc25"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the optimal threshold\n",
    "opt = threshold[score.index(max(score))]\n",
    "opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UF1mHdE3rjVu"
   },
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_74ujVjmVlcT"
   },
   "outputs": [],
   "source": [
    "#predictions for optimal threshold\n",
    "y_pred_seq = classify(pred_prob,opt)\n",
    "y_pred = np.array(y_pred_seq).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1508,
     "status": "ok",
     "timestamp": 1584605984432,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "9LKB6W7tItUm",
    "outputId": "f81067e9-5c47-44ca-b0e4-970b1b1142dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.90     17520\n",
      "           1       0.63      0.70      0.66      4700\n",
      "\n",
      "    accuracy                           0.85     22220\n",
      "   macro avg       0.78      0.79      0.78     22220\n",
      "weighted avg       0.86      0.85      0.85     22220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No improvement over the LSTM model but ofthen GRU beats LSTM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tzQsUoEV7ldm"
   },
   "outputs": [],
   "source": [
    "y_pred = mlb.inverse_transform(np.array(y_pred_seq))\n",
    "y_true = mlb.inverse_transform(np.array(y_val))\n",
    "\n",
    "df = pd.DataFrame({'comment':x_val,'actual':y_true,'predictions':y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1402,
     "status": "ok",
     "timestamp": 1584606015578,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "plf_uclDlxwL",
    "outputId": "09ec6a82-9e69-4981-be00-38a4236b8e14"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>actual</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1804</th>\n",
       "      <td>r studio hi i m running lda on a dataset with observations classes and variables my goal is to create a classification model using the lda function after loading my variables i receive a warning t...</td>\n",
       "      <td>(classification, r)</td>\n",
       "      <td>(classification, machine learning)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>i don t know if this is a suitable question here i have data from groups neutral and positive here how the data looks like a b c null d null e the first column is the record name id and the last c...</td>\n",
       "      <td>(classification, machine learning)</td>\n",
       "      <td>(classification, machine learning)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>i have a time series data in day format of places for days stored as a matrix the structure of data is meter daywise structure c dim c l l dimnames list c apfac apfac apfac apfac apfac c d d d d d...</td>\n",
       "      <td>(r, time series)</td>\n",
       "      <td>(r, time series)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764</th>\n",
       "      <td>using stata i would like to develop analytic models that could be implemented by school administrators to flag students for intervention i m wondering if it would be possible to develop these mode...</td>\n",
       "      <td>(logistic, probability, regression)</td>\n",
       "      <td>(logistic, r, regression)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>say i m working with a biological system where two or more genotypes are reared under a series of daylength treatments and scored for a binomial response variable y i want to know whether the geno...</td>\n",
       "      <td>(logistic, regression)</td>\n",
       "      <td>(logistic, regression)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>suppose there are golf balls in an urn and you draw balls in stages without replacement first you draw then and then the remaining how many ways are there to do this i believe the answer would be ...</td>\n",
       "      <td>(probability, self study)</td>\n",
       "      <td>(distributions, probability, self study)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>i measured one response variable y as a function of two measured independent variables x and x it is common practice in my field of research to transform y into another response variable y x y the...</td>\n",
       "      <td>(r, regression)</td>\n",
       "      <td>(logistic, r, regression)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1925</th>\n",
       "      <td>i have been looking into the random trees and decision trees in opencv and something that still confuses me is the differences between regression and classification can anyone outline what the dif...</td>\n",
       "      <td>(classification, regression)</td>\n",
       "      <td>(classification, logistic, machine learning, regression)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1654</th>\n",
       "      <td>my data set looks like this id weak year strong year covariate covariate na na na na so each object can produce either a weak reaction a strong reaction or a weak and then strong reaction weak yea...</td>\n",
       "      <td>(r, time series)</td>\n",
       "      <td>(r, regression)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1515</th>\n",
       "      <td>i m trying to determine what type of analysis i should use in r for my data question i m thinking some type of time lag or time series analysis my question is how the duration of a behavior change...</td>\n",
       "      <td>(r, time series)</td>\n",
       "      <td>(r, self study)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                      comment  ...                                               predictions\n",
       "1804  r studio hi i m running lda on a dataset with observations classes and variables my goal is to create a classification model using the lda function after loading my variables i receive a warning t...  ...                        (classification, machine learning)\n",
       "1537  i don t know if this is a suitable question here i have data from groups neutral and positive here how the data looks like a b c null d null e the first column is the record name id and the last c...  ...                        (classification, machine learning)\n",
       "309   i have a time series data in day format of places for days stored as a matrix the structure of data is meter daywise structure c dim c l l dimnames list c apfac apfac apfac apfac apfac c d d d d d...  ...                                          (r, time series)\n",
       "1764  using stata i would like to develop analytic models that could be implemented by school administrators to flag students for intervention i m wondering if it would be possible to develop these mode...  ...                                 (logistic, r, regression)\n",
       "1539  say i m working with a biological system where two or more genotypes are reared under a series of daylength treatments and scored for a binomial response variable y i want to know whether the geno...  ...                                    (logistic, regression)\n",
       "220   suppose there are golf balls in an urn and you draw balls in stages without replacement first you draw then and then the remaining how many ways are there to do this i believe the answer would be ...  ...                  (distributions, probability, self study)\n",
       "44    i measured one response variable y as a function of two measured independent variables x and x it is common practice in my field of research to transform y into another response variable y x y the...  ...                                 (logistic, r, regression)\n",
       "1925  i have been looking into the random trees and decision trees in opencv and something that still confuses me is the differences between regression and classification can anyone outline what the dif...  ...  (classification, logistic, machine learning, regression)\n",
       "1654  my data set looks like this id weak year strong year covariate covariate na na na na so each object can produce either a weak reaction a strong reaction or a weak and then strong reaction weak yea...  ...                                           (r, regression)\n",
       "1515  i m trying to determine what type of analysis i should use in r for my data question i m thinking some type of time lag or time series analysis my question is how the duration of a behavior change...  ...                                           (r, self study)\n",
       "\n",
       "[10 rows x 3 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K868o9U3H5u0"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xG_KIc1xhl2V"
   },
   "outputs": [],
   "source": [
    "def predict_tag(comment):  \n",
    "  text=[]\n",
    "\n",
    "  #preprocess  \n",
    "  text = [cleaner(comment)]\n",
    "\n",
    "  #convert to integer sequences\n",
    "  seq = x_tokenizer.texts_to_sequences(text)\n",
    "\n",
    "  #pad the sequence\n",
    "  pad_seq = pad_sequences(seq,  padding='post', maxlen=max_len)\n",
    "\n",
    "  #make predictions\n",
    "  pred_prob = model.predict(pad_seq)\n",
    "  classes = classify(pred_prob,opt)[0]\n",
    "  \n",
    "  classes = np.array([classes])\n",
    "  classes = mlb.inverse_transform(classes)  \n",
    "  return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1442,
     "status": "ok",
     "timestamp": 1584606050071,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "0Al3kfEgEYhU",
    "outputId": "176ac5ee-2f52-4c7f-88cd-c7596dd08ecf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment: For example, in the case of logistic regression, the learning function is a Sigmoid function that tries to separate the 2 classes\n",
      "Predicted Tags: [('classification', 'logistic', 'machine learning', 'regression')]\n"
     ]
    }
   ],
   "source": [
    "comment = \"For example, in the case of logistic regression, the learning function is a Sigmoid function that tries to separate the 2 classes\"\n",
    "\n",
    "print(\"Comment:\",comment)\n",
    "print(\"Predicted Tags:\",predict_tag(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kUSrb3nE-r6S"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "auto_tagging_system_GRU_v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
