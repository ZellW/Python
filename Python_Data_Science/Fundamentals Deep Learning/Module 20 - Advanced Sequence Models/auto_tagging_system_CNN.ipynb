{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "L_axH7-PrUFT",
    "outputId": "be411fd9-91ec-4d7e-a382-d579fd62d7e1"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WtPKZPpFxD6g"
   },
   "source": [
    "# Objective\n",
    "\n",
    "Build a model to automatically predict tags for a given a StackExchange question by using the text of the question.\n",
    "![alt text](https://cdn.sstatic.net/Sites/stackoverflow/company/img/logos/se/se-logo.svg?v=d29f0785ebb7)\n",
    "\n",
    "__Dataset Specs__: Over 85,000 questions\n",
    "\n",
    "[Download Link](https://www.kaggle.com/stackoverflow/statsquestions#Questions.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HfaOlnHpy-Va"
   },
   "source": [
    "# Steps to Follow\n",
    "\n",
    "\n",
    "\n",
    "1. Load Data and Import Libraries\n",
    "2. Text Cleaning\n",
    "3. Merge Tags with Questions\n",
    "4. Dataset Prepartion\n",
    "5. Text Representation\n",
    "6. Model Building\n",
    "    1. Define Model Architecture\n",
    "    2. Train the Model\n",
    "7. Model Predictions\n",
    "8. Model Evaluation\n",
    "9. Inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uVFrjcv4H_pa"
   },
   "source": [
    "# Load Data and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m_fivLMXrYDM"
   },
   "outputs": [],
   "source": [
    "# extract data from the ZIP file\n",
    "# !unzip '/content/drive/statsquestions.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gitKl0VQbmBN"
   },
   "outputs": [],
   "source": [
    "#string matching\n",
    "import re \n",
    "\n",
    "#reading files\n",
    "import pandas as pd\n",
    "\n",
    "#handling html data\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#visualization\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qLXg0bRfclmw"
   },
   "outputs": [],
   "source": [
    "# load the stackoverflow questions dataset\n",
    "questions_df = pd.read_csv('../../../../../LargeData/Analytics_Vidhya/StackOverflow_tagging/Questions.csv',encoding='latin-1')\n",
    "\n",
    "# load the tags dataset\n",
    "tags_df = pd.read_csv('../../../../../LargeData/Analytics_Vidhya/StackOverflow_tagging/Tags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "8pxycLMRKvO4",
    "outputId": "16e22f44-1a92-4cb3-ee6d-0fda7a4bf242"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Score</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2010-07-19T19:14:44Z</td>\n",
       "      <td>272</td>\n",
       "      <td>The Two Cultures: statistics vs. machine learning?</td>\n",
       "      <td>&lt;p&gt;Last year, I read a blog post from &lt;a href=\"http://anyall.org/\"&gt;Brendan O'Connor&lt;/a&gt; entitled &lt;a href=\"http://anyall.org/blog/2008/12/statistics-vs-machine-learning-fight/\"&gt;\"Statistics vs. Mach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2010-07-19T19:24:36Z</td>\n",
       "      <td>4</td>\n",
       "      <td>Forecasting demographic census</td>\n",
       "      <td>&lt;p&gt;What are some of the ways to forecast demographic census with some validation and calibration techniques?&lt;/p&gt;\\n\\n&lt;p&gt;Some of the concerns:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Census blocks vary in sizes as rural\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>66.0</td>\n",
       "      <td>2010-07-19T19:25:39Z</td>\n",
       "      <td>208</td>\n",
       "      <td>Bayesian and frequentist reasoning in plain English</td>\n",
       "      <td>&lt;p&gt;How would you describe in plain English the characteristics that distinguish Bayesian from Frequentist reasoning?&lt;/p&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2010-07-19T19:28:44Z</td>\n",
       "      <td>138</td>\n",
       "      <td>What is the meaning of p values and t values in statistical tests?</td>\n",
       "      <td>&lt;p&gt;After taking a statistics course and then trying to help fellow students, I noticed one subject that inspires much head-desk banging is interpreting the results of statistical hypothesis tests....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2010-07-19T19:31:47Z</td>\n",
       "      <td>58</td>\n",
       "      <td>Examples for teaching: Correlation does not mean causation</td>\n",
       "      <td>&lt;p&gt;There is an old saying: \"Correlation does not mean causation\". When I teach, I tend to use the following standard examples to illustrate this point:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;number of storks and birth ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  OwnerUserId          CreationDate  Score  \\\n",
       "0   6          5.0  2010-07-19T19:14:44Z    272   \n",
       "1  21         59.0  2010-07-19T19:24:36Z      4   \n",
       "2  22         66.0  2010-07-19T19:25:39Z    208   \n",
       "3  31         13.0  2010-07-19T19:28:44Z    138   \n",
       "4  36          8.0  2010-07-19T19:31:47Z     58   \n",
       "\n",
       "                                                                Title  \\\n",
       "0                  The Two Cultures: statistics vs. machine learning?   \n",
       "1                                      Forecasting demographic census   \n",
       "2                 Bayesian and frequentist reasoning in plain English   \n",
       "3  What is the meaning of p values and t values in statistical tests?   \n",
       "4          Examples for teaching: Correlation does not mean causation   \n",
       "\n",
       "                                                                                                                                                                                                      Body  \n",
       "0  <p>Last year, I read a blog post from <a href=\"http://anyall.org/\">Brendan O'Connor</a> entitled <a href=\"http://anyall.org/blog/2008/12/statistics-vs-machine-learning-fight/\">\"Statistics vs. Mach...  \n",
       "1  <p>What are some of the ways to forecast demographic census with some validation and calibration techniques?</p>\\n\\n<p>Some of the concerns:</p>\\n\\n<ul>\\n<li>Census blocks vary in sizes as rural\\n...  \n",
       "2                                                                               <p>How would you describe in plain English the characteristics that distinguish Bayesian from Frequentist reasoning?</p>\\n  \n",
       "3  <p>After taking a statistics course and then trying to help fellow students, I noticed one subject that inspires much head-desk banging is interpreting the results of statistical hypothesis tests....  \n",
       "4  <p>There is an old saying: \"Correlation does not mean causation\". When I teach, I tend to use the following standard examples to illustrate this point:</p>\\n\\n<ol>\\n<li>number of storks and birth ...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print first 5 rows\n",
    "questions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>bayesian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>prior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>elicitation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>distributions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>normality</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id            Tag\n",
       "0   1       bayesian\n",
       "1   1          prior\n",
       "2   1    elicitation\n",
       "3   2  distributions\n",
       "4   2      normality"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "siBpKFe3IZNw"
   },
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hmeuzAF9Ic1d"
   },
   "source": [
    "Let's define a function to clean the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ADh9l-RWNSDU"
   },
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "\n",
    "  text = BeautifulSoup(text).get_text()\n",
    "  \n",
    "  # fetch alphabetic characters\n",
    "  text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "\n",
    "  # convert text to lower case\n",
    "  text = text.lower()\n",
    "\n",
    "  # split text into tokens to remove whitespaces\n",
    "  tokens = text.split()\n",
    "\n",
    "  return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o98nUFycNSB-"
   },
   "outputs": [],
   "source": [
    "# call preprocessing function\n",
    "questions_df['cleaned_text'] = questions_df['Body'].apply(cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "umvS_4ZQNR6r",
    "outputId": "dff3f8d9-f908-407f-ca37-05fb41205c76"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<p>What are some of the ways to forecast demographic census with some validation and calibration techniques?</p>\\n\\n<p>Some of the concerns:</p>\\n\\n<ul>\\n<li>Census blocks vary in sizes as rural\\nareas are a lot larger than condensed\\nurban areas. Is there a need to account for the area size difference?</li>\\n<li>if let's say I have census data\\ndating back to 4 - 5 census periods,\\nhow far can i forecast it into the\\nfuture?</li>\\n<li>if some of the census zone change\\nlightly in boundaries, how can i\\naccount for that change?</li>\\n<li>What are the methods to validate\\ncensus forecasts? for example, if i\\nhave data for existing 5 census\\nperiods, should I model the first 3\\nand test it on the latter two? or is\\nthere another way?</li>\\n<li>what's the state of practice in\\nforecasting census data, and what are\\nsome of the state of the art methods?</li>\\n</ul>\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_df['Body'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "FoEZ0aQ7KvN2",
    "outputId": "7f95fabb-029a-4011-a5b6-8232b6cc7902"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what are some of the ways to forecast demographic census with some validation and calibration techniques some of the concerns census blocks vary in sizes as rural areas are a lot larger than condensed urban areas is there a need to account for the area size difference if let s say i have census data dating back to census periods how far can i forecast it into the future if some of the census zone change lightly in boundaries how can i account for that change what are the methods to validate census forecasts for example if i have data for existing census periods should i model the first and test it on the latter two or is there another way what s the state of practice in forecasting census data and what are some of the state of the art methods'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_df['cleaned_text'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dTPISO70LOM4"
   },
   "source": [
    "# Merge Tags with Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jTXwGw2xKhWk"
   },
   "source": [
    "Let's now explore the tags data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "4p9JAFA9tTxO",
    "outputId": "6671d78b-5508-4822-9c68-144607d44ace"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>bayesian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>prior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>elicitation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>distributions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>normality</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id            Tag\n",
       "0   1       bayesian\n",
       "1   1          prior\n",
       "2   1    elicitation\n",
       "3   2  distributions\n",
       "4   2      normality"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rVGubRcWe01J",
    "outputId": "6c0d65c6-56e6-45a0-d8a3-a466d2a5fc6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1315"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count of unique tags\n",
    "len(tags_df['Tag'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "ZqW5nBknesV-",
    "outputId": "c4b96473-50c1-4bce-81b0-ee0f8c541922"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r                   13236\n",
       "regression          10959\n",
       "machine-learning     6089\n",
       "time-series          5559\n",
       "probability          4217\n",
       "                    ...  \n",
       "network-layout          1\n",
       "concept-drift           1\n",
       "replicability           1\n",
       "qsar                    1\n",
       "shapley-value           1\n",
       "Name: Tag, Length: 1315, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_df['Tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3SEjB4OwiJ0b"
   },
   "outputs": [],
   "source": [
    "# remove \"-\" from the tags\n",
    "tags_df['Tag']= tags_df['Tag'].apply(lambda x:re.sub(\"-\",\" \",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "MhPvTj6ytcW0",
    "outputId": "b6350a91-692e-4ab2-b530-cddbc6d039cf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[bayesian, prior, elicitation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[distributions, normality]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[software, open source]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[distributions, statistical significance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>[machine learning]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                       tags\n",
       "0   1             [bayesian, prior, elicitation]\n",
       "1   2                 [distributions, normality]\n",
       "2   3                    [software, open source]\n",
       "3   4  [distributions, statistical significance]\n",
       "4   6                         [machine learning]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group tags Id wise\n",
    "tags_df = tags_df.groupby('Id').apply(lambda x:x['Tag'].values).reset_index(name='tags')\n",
    "tags_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zCnEKm1ouTnm"
   },
   "outputs": [],
   "source": [
    "# merge tags and questions\n",
    "df = pd.merge(questions_df,tags_df,how='inner',on='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "fn37BQ5TusSt",
    "outputId": "1c9f0763-d3b9-4031-e9cf-c02e8d42db73"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Body</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>&lt;p&gt;Last year, I read a blog post from &lt;a href=\"http://anyall.org/\"&gt;Brendan O'Connor&lt;/a&gt; entitled &lt;a href=\"http://anyall.org/blog/2008/12/statistics-vs-machine-learning-fight/\"&gt;\"Statistics vs. Mach...</td>\n",
       "      <td>last year i read a blog post from brendan o connor entitled statistics vs machine learning fight that discussed some of the differences between the two fields andrew gelman responded favorably to ...</td>\n",
       "      <td>[machine learning]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>&lt;p&gt;What are some of the ways to forecast demographic census with some validation and calibration techniques?&lt;/p&gt;\\n\\n&lt;p&gt;Some of the concerns:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Census blocks vary in sizes as rural\\n...</td>\n",
       "      <td>what are some of the ways to forecast demographic census with some validation and calibration techniques some of the concerns census blocks vary in sizes as rural areas are a lot larger than conde...</td>\n",
       "      <td>[forecasting, population, census]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>&lt;p&gt;How would you describe in plain English the characteristics that distinguish Bayesian from Frequentist reasoning?&lt;/p&gt;\\n</td>\n",
       "      <td>how would you describe in plain english the characteristics that distinguish bayesian from frequentist reasoning</td>\n",
       "      <td>[bayesian, frequentist]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>&lt;p&gt;After taking a statistics course and then trying to help fellow students, I noticed one subject that inspires much head-desk banging is interpreting the results of statistical hypothesis tests....</td>\n",
       "      <td>after taking a statistics course and then trying to help fellow students i noticed one subject that inspires much head desk banging is interpreting the results of statistical hypothesis tests it s...</td>\n",
       "      <td>[hypothesis testing, t test, p value, interpretation, intuition]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36</td>\n",
       "      <td>&lt;p&gt;There is an old saying: \"Correlation does not mean causation\". When I teach, I tend to use the following standard examples to illustrate this point:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;number of storks and birth ...</td>\n",
       "      <td>there is an old saying correlation does not mean causation when i teach i tend to use the following standard examples to illustrate this point number of storks and birth rate in denmark number of ...</td>\n",
       "      <td>[correlation, teaching]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  \\\n",
       "0   6   \n",
       "1  21   \n",
       "2  22   \n",
       "3  31   \n",
       "4  36   \n",
       "\n",
       "                                                                                                                                                                                                      Body  \\\n",
       "0  <p>Last year, I read a blog post from <a href=\"http://anyall.org/\">Brendan O'Connor</a> entitled <a href=\"http://anyall.org/blog/2008/12/statistics-vs-machine-learning-fight/\">\"Statistics vs. Mach...   \n",
       "1  <p>What are some of the ways to forecast demographic census with some validation and calibration techniques?</p>\\n\\n<p>Some of the concerns:</p>\\n\\n<ul>\\n<li>Census blocks vary in sizes as rural\\n...   \n",
       "2                                                                               <p>How would you describe in plain English the characteristics that distinguish Bayesian from Frequentist reasoning?</p>\\n   \n",
       "3  <p>After taking a statistics course and then trying to help fellow students, I noticed one subject that inspires much head-desk banging is interpreting the results of statistical hypothesis tests....   \n",
       "4  <p>There is an old saying: \"Correlation does not mean causation\". When I teach, I tend to use the following standard examples to illustrate this point:</p>\\n\\n<ol>\\n<li>number of storks and birth ...   \n",
       "\n",
       "                                                                                                                                                                                              cleaned_text  \\\n",
       "0  last year i read a blog post from brendan o connor entitled statistics vs machine learning fight that discussed some of the differences between the two fields andrew gelman responded favorably to ...   \n",
       "1  what are some of the ways to forecast demographic census with some validation and calibration techniques some of the concerns census blocks vary in sizes as rural areas are a lot larger than conde...   \n",
       "2                                                                                         how would you describe in plain english the characteristics that distinguish bayesian from frequentist reasoning   \n",
       "3  after taking a statistics course and then trying to help fellow students i noticed one subject that inspires much head desk banging is interpreting the results of statistical hypothesis tests it s...   \n",
       "4  there is an old saying correlation does not mean causation when i teach i tend to use the following standard examples to illustrate this point number of storks and birth rate in denmark number of ...   \n",
       "\n",
       "                                                               tags  \n",
       "0                                                [machine learning]  \n",
       "1                                 [forecasting, population, census]  \n",
       "2                                           [bayesian, frequentist]  \n",
       "3  [hypothesis testing, t test, p value, interpretation, intuition]  \n",
       "4                                           [correlation, teaching]  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['Id','Body','cleaned_text','tags']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2cqkOjieMKBk",
    "outputId": "a30ce99a-47b3-4b52-c85b-af38fe925e95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85085, 4)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YV6KXBw1MRyY"
   },
   "source": [
    "There are over 85,000 unique questions and over 1300 tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c_22ook_MiRv"
   },
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3uj_0l0jwL3f"
   },
   "outputs": [],
   "source": [
    "# check frequency of occurence of each tag\n",
    "freq= {}\n",
    "for i in df['tags']:\n",
    "  for j in i:\n",
    "    if j in freq.keys():\n",
    "      freq[j] = freq[j] + 1\n",
    "    else:\n",
    "      freq[j] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CUDiameNNhPh"
   },
   "source": [
    "Let's find out the most frequent tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WOzWyGfLzli9"
   },
   "outputs": [],
   "source": [
    "# sort the dictionary in descending order\n",
    "freq = dict(sorted(freq.items(), key=lambda x:x[1],reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "vKcagvyi0Wfk",
    "outputId": "50609cc4-95f6-4530-cefc-17124143c3d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'r': 13236,\n",
       " 'regression': 10959,\n",
       " 'machine learning': 6089,\n",
       " 'time series': 5559,\n",
       " 'probability': 4217,\n",
       " 'hypothesis testing': 3869,\n",
       " 'self study': 3732,\n",
       " 'distributions': 3501,\n",
       " 'logistic': 3316,\n",
       " 'classification': 2881}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# freq.items() # too much data returned\n",
    "\n",
    "# Simple way to slice a dictionary\n",
    "import itertools\n",
    "\n",
    "dict(itertools.islice(freq.items(), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "1PQMi8WIv0_u",
    "outputId": "55217790-f1fd-45cb-b180-75ede24c0b42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['r',\n",
       " 'regression',\n",
       " 'machine learning',\n",
       " 'time series',\n",
       " 'probability',\n",
       " 'hypothesis testing',\n",
       " 'self study',\n",
       " 'distributions',\n",
       " 'logistic',\n",
       " 'classification']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 most frequent tags\n",
    "common_tags = list(freq.keys())[:10]\n",
    "common_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "buPS2OrlN50F"
   },
   "source": [
    "We will use only those questions/queries that have the above 10 tags associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CVB3DKppym51"
   },
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "\n",
    "for i in range(len(df['tags'])):\n",
    "  \n",
    "  temp=[]\n",
    "  for j in df['tags'][i]:\n",
    "    if j in common_tags:\n",
    "      temp.append(j)\n",
    "\n",
    "  if(len(temp)>1):\n",
    "    x.append(df['cleaned_text'][i])\n",
    "    y.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fYMxubGzzN7Q",
    "outputId": "078a8a95-c731-4bd8-fca3-1bbeb52e8c3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11106"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of questions left\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "hFmgSxWp00Gu",
    "outputId": "0760f296-79c8-4820-d222-974c29e015ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['r', 'time series'],\n",
       " ['regression', 'distributions'],\n",
       " ['distributions', 'probability', 'hypothesis testing'],\n",
       " ['hypothesis testing', 'self study'],\n",
       " ['r', 'regression', 'time series'],\n",
       " ['r', 'time series', 'self study'],\n",
       " ['probability', 'hypothesis testing'],\n",
       " ['r', 'regression'],\n",
       " ['r', 'regression'],\n",
       " ['regression', 'logistic']]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C0okjkZUPEBJ"
   },
   "source": [
    "We will the input sequences to our model to the length of 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pzCk5T-KR-W5",
    "outputId": "6f34b21b-2345-4d13-88ee-5e422ba1a9cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11106, 10)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    " \n",
    "y = mlb.fit_transform(y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eJxESqMJSEKZ",
    "outputId": "ad4d83cc-5d17-435d-89f4-6a0a46b11419"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "YZHXV-GqSg3s",
    "outputId": "39d4da92-4e3b-43d1-d789-e75e108075c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['classification', 'distributions', 'hypothesis testing',\n",
       "       'logistic', 'machine learning', 'probability', 'r', 'regression',\n",
       "       'self study', 'time series'], dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WkF4pDaJStjg"
   },
   "source": [
    "We can now split the dataset into training set and validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QPHeAiS9KvD9"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_tr,x_val,y_tr,y_val=train_test_split(x, y, test_size=0.2, random_state=0,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H4GaDNwtTmPh"
   },
   "source": [
    "# Text Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TtrDalDiPXJM"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "\n",
    "#prepare a tokenizer\n",
    "x_tokenizer = Tokenizer() \n",
    "\n",
    "#prepare vocabulary\n",
    "x_tokenizer.fit_on_texts(x_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zl1vZKpwU2P2",
    "outputId": "f4270be5-48cc-4c08-bd4c-994287162783"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'i': 2,\n",
       " 'to': 3,\n",
       " 'a': 4,\n",
       " 'of': 5,\n",
       " 'is': 6,\n",
       " 'and': 7,\n",
       " 'in': 8,\n",
       " 'l': 9,\n",
       " 'x': 10,\n",
       " 'for': 11,\n",
       " 'that': 12,\n",
       " 'data': 13,\n",
       " 'this': 14,\n",
       " 't': 15,\n",
       " 'have': 16,\n",
       " 'y': 17,\n",
       " 'with': 18,\n",
       " 'model': 19,\n",
       " 'it': 20}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_tokenizer.word_index # returns too much data\n",
    "\n",
    "# List the first 20 items in the dictionary.  Note the numbers is the row, not count of occurence!\n",
    "dict(itertools.islice(x_tokenizer.word_index.items(), 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Srz9RWKvVsm7",
    "outputId": "7a352d22-1431-4c9a-c135-8e00e78d490f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25315"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nQLwSve9VYqN"
   },
   "source": [
    "There are around 25,000 tokens in the training dataset. Let's see how many tokens appear at least 5 times in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "H-d_UjVmPjgo",
    "outputId": "7ec90b8c-3b52-4d53-e298-ad0e16b9881e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12575\n"
     ]
    }
   ],
   "source": [
    "thresh = 3\n",
    "\n",
    "cnt=0\n",
    "for key,value in x_tokenizer.word_counts.items():\n",
    "  if value>=thresh:\n",
    "    cnt=cnt+1\n",
    "\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Eqly3dnVh-S"
   },
   "source": [
    "Over 12,000 tokens have appeared three times or more in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FZrJnr-dItPn"
   },
   "outputs": [],
   "source": [
    "# prepare the tokenizer again\n",
    "x_tokenizer = Tokenizer(num_words=cnt,oov_token='unk')\n",
    "\n",
    "#prepare vocabulary\n",
    "x_tokenizer.fit_on_texts(x_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJFfvLDJWZbb"
   },
   "source": [
    "Now that we have encoded every token to an integer, let's convert the text sequences to integer sequences. After that we will pad the integer sequences to the maximum sequence length, i.e., 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VpJvPYx5WR07"
   },
   "outputs": [],
   "source": [
    "#define threshold for maximum length of a setence\n",
    "max_len=100\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "x_tr_seq = x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val_seq = x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "#padding up with zero \n",
    "x_tr_seq = pad_sequences(x_tr_seq,  padding='post', maxlen=max_len)\n",
    "x_val_seq = pad_sequences(x_val_seq, padding='post', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6TkeQpLgXled"
   },
   "source": [
    "Since we are padding the sequences with zeros, we must increment the vocabulary size by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Q4na29hBItes",
    "outputId": "7ceb399f-ac42-42ec-84e0-16036bcf99fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12576"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no. of unique words\n",
    "x_voc_size = x_tokenizer.num_words + 1\n",
    "x_voc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "A9jdhRc_O12J",
    "outputId": "67a68b9a-2ef3-4857-d835-3740026ed06b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1953, 5711,  416, 2023,    1,  226, 1747, 3740,  609,   43,  181,\n",
       "       1953,  372,   19,  100,  416,    9, 1747, 3839,  238,   27,   27,\n",
       "         27,   27,   27,   70,    6, 6919,    8, 1163,   70,    6,   43,\n",
       "         43, 1802, 1802, 1802,   36,   36,   36,   36, 4308, 5410,    4,\n",
       "        124,  592,  107,   22,    2, 1747, 4065,   27,   10, 1309,   10,\n",
       "       6415,   10,  190,   10,  416,   10,   27,   10, 1309,   10, 6415,\n",
       "         10,  190,   10,  416,   10,  456,  139,   15,    7,    2, 4610,\n",
       "        164,   27,   10, 1309,   10, 6415,   10,  190,   10,  416,   10,\n",
       "         27,   76,   27, 1309,   76,   27, 6415,   76,   27,  190,   76,\n",
       "         27])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr_seq[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BCtPnSrsscN1"
   },
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RrRgqQ4M8OZu"
   },
   "outputs": [],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxE6IK3Uic-d"
   },
   "source": [
    "### Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "vHtamwcMkVcr",
    "outputId": "f4a3c675-6459-46b5-b725-b19ac3b10879"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 50)           628800    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 100, 64)           9664      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100, 64)           0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 648,074\n",
      "Trainable params: 648,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model architecture\n",
    "K.clear_session()\n",
    "model =  Sequential()\n",
    "model.add(Embedding(x_voc_size, 50, trainable=True, input_shape=(max_len,)))  #embedding layer\n",
    "  \n",
    "model.add(Conv1D(64,3,padding='same'))  #conv1d layer: 64 filters and 3 is height of filter and \n",
    "# this is what what feature maps as output\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(GlobalMaxPooling1D()) # applies max pooling on the feature maps\n",
    "  \n",
    "model.add(Dense(128,activation='relu'))  #dense layer\n",
    "\n",
    "model.add(Dense(10,activation='sigmoid')) #output layer\n",
    "model.summary() #summary) of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tzRoTFVIItjK"
   },
   "outputs": [],
   "source": [
    "#define optimizer and loss\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy')\n",
    "\n",
    "#checkpoint to save best model during training\n",
    "mc = ModelCheckpoint(\"weights.best.hdf5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "80gtKbElii0e"
   },
   "source": [
    "### Train the Model\n",
    "\n",
    "This is MUCH faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "colab_type": "code",
    "id": "XH8ggzcdkzkL",
    "outputId": "1b3a5785-5ecd-4985-f463-4cb66b67d135"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.1258\n",
      "Epoch 00001: val_loss did not improve from 0.26730\n",
      "70/70 [==============================] - 2s 26ms/step - loss: 0.1260 - val_loss: 0.2889\n",
      "Epoch 2/10\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.1079\n",
      "Epoch 00002: val_loss did not improve from 0.26730\n",
      "70/70 [==============================] - 2s 25ms/step - loss: 0.1081 - val_loss: 0.2993\n",
      "Epoch 3/10\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.0918\n",
      "Epoch 00003: val_loss did not improve from 0.26730\n",
      "70/70 [==============================] - 2s 25ms/step - loss: 0.0919 - val_loss: 0.3155\n",
      "Epoch 4/10\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.0780\n",
      "Epoch 00004: val_loss did not improve from 0.26730\n",
      "70/70 [==============================] - 2s 26ms/step - loss: 0.0780 - val_loss: 0.3297\n",
      "Epoch 5/10\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.0650\n",
      "Epoch 00005: val_loss did not improve from 0.26730\n",
      "70/70 [==============================] - 2s 25ms/step - loss: 0.0650 - val_loss: 0.3468\n",
      "Epoch 6/10\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0544\n",
      "Epoch 00006: val_loss did not improve from 0.26730\n",
      "70/70 [==============================] - 2s 24ms/step - loss: 0.0543 - val_loss: 0.3653\n",
      "Epoch 7/10\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.0465\n",
      "Epoch 00007: val_loss did not improve from 0.26730\n",
      "70/70 [==============================] - 2s 25ms/step - loss: 0.0467 - val_loss: 0.3821\n",
      "Epoch 8/10\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.0391\n",
      "Epoch 00008: val_loss did not improve from 0.26730\n",
      "70/70 [==============================] - 2s 25ms/step - loss: 0.0391 - val_loss: 0.3978\n",
      "Epoch 9/10\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.0324\n",
      "Epoch 00009: val_loss did not improve from 0.26730\n",
      "70/70 [==============================] - 2s 25ms/step - loss: 0.0324 - val_loss: 0.4198\n",
      "Epoch 10/10\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.0273\n",
      "Epoch 00010: val_loss did not improve from 0.26730\n",
      "70/70 [==============================] - 2s 25ms/step - loss: 0.0274 - val_loss: 0.4352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b3ac0870a0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model \n",
    "model.fit(x_tr_seq, y_tr, batch_size=128, epochs=10, verbose=1, validation_data=(x_val_seq, y_val), callbacks=[mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JDzen8xvioUd"
   },
   "source": [
    "# Model Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xL8qz8zvDvpH"
   },
   "outputs": [],
   "source": [
    "# load weights into new model\n",
    "model.load_weights(\"weights.best.hdf5\")\n",
    "\n",
    "#predict probabilities\n",
    "pred_prob = model.predict(x_val_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "AUF5H7bIlRLr",
    "outputId": "635d51c8-cfd5-479e-96ed-fc417224fe42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05173081, 0.05248803, 0.00771904, 0.09648392, 0.24082986,\n",
       "       0.04391071, 0.9517    , 0.47870407, 0.03080544, 0.05613467],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_prob[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ph2xAWbFjLv5"
   },
   "source": [
    "The predictions are in terms of probabilities for each of the 10 tags. Hence we need to have a threshold value to convert these probabilities to 0 or 1.\n",
    "\n",
    "Let's specify a set of candidate threshold values. We will select the threshold value that performs the best for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "5hYDCTKXguMF",
    "outputId": "f2f3a219-79cf-41d4-9d77-d752fd7b93a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ,\n",
       "       0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21,\n",
       "       0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32,\n",
       "       0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43,\n",
       "       0.44, 0.45, 0.46, 0.47, 0.48, 0.49])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#define candidate threshold values\n",
    "threshold  = np.arange(0,0.5,0.01)\n",
    "threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NA6wMIewkICl"
   },
   "source": [
    "Let's define a function that takes a threshold value and uses it to convert probabilities into 1 or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aay56TvDGPoX"
   },
   "outputs": [],
   "source": [
    "# convert probabilities into classes or tags based on a threshold value\n",
    "def classify(pred_prob,thresh):\n",
    "  y_pred_seq = []\n",
    "\n",
    "  for i in pred_prob:\n",
    "    temp=[]\n",
    "    for j in i:\n",
    "      if j>=thresh:\n",
    "        temp.append(1)\n",
    "      else:\n",
    "        temp.append(0)\n",
    "    y_pred_seq.append(temp)\n",
    "\n",
    "  return y_pred_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s0auJJmNDtv9"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "score=[]\n",
    "\n",
    "#convert to 1 array\n",
    "y_true = np.array(y_val).ravel() \n",
    "\n",
    "for thresh in threshold:\n",
    "    \n",
    "    #classes for each threshold\n",
    "    y_pred_seq = classify(pred_prob,thresh) \n",
    "\n",
    "    #convert to 1d array\n",
    "    y_pred = np.array(y_pred_seq).ravel()\n",
    "\n",
    "    score.append(metrics.f1_score(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jrA8nJIGVBsl",
    "outputId": "e4e4caa8-90b2-4e7c-a32a-76a6c94cc0a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the optimal threshold\n",
    "opt = threshold[score.index(max(score))]\n",
    "opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UF1mHdE3rjVu"
   },
   "source": [
    "# Model Evaluation\n",
    "\n",
    "macro avg is 0.83.  This outperfomrs the others!!  This is not always the case but it works better this time.  For auto tagging, CNNs can be the champion.\n",
    "\n",
    "Surely CNN can perform better in a few cases than RNN but this is not always true. For problems like machine translation and text summarization RNN performs significantly well. Both models have their respective advantages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_74ujVjmVlcT"
   },
   "outputs": [],
   "source": [
    "#predictions for optimal threshold\n",
    "y_pred_seq = classify(pred_prob,opt)\n",
    "y_pred = np.array(y_pred_seq).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "9LKB6W7tItUm",
    "outputId": "e0a6e61e-7e4a-45b2-a47d-7a22c9352c47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93     17520\n",
      "           1       0.75      0.72      0.74      4700\n",
      "\n",
      "    accuracy                           0.89     22220\n",
      "   macro avg       0.84      0.83      0.83     22220\n",
      "weighted avg       0.89      0.89      0.89     22220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tzQsUoEV7ldm"
   },
   "outputs": [],
   "source": [
    "y_pred = mlb.inverse_transform(np.array(y_pred_seq))\n",
    "y_true = mlb.inverse_transform(np.array(y_val))\n",
    "\n",
    "df = pd.DataFrame({'comment':x_val,'actual':y_true,'predictions':y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "colab_type": "code",
    "id": "plf_uclDlxwL",
    "outputId": "c8ce831a-bc70-48d5-edbd-5c9574f0bb8d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>actual</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>is there any function for m estimation in multivariate linear regression model in r i can estimate the beta s in my model by using the rlm by rewriting the y variables into one column but i would ...</td>\n",
       "      <td>(r, regression)</td>\n",
       "      <td>(r, regression)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554</th>\n",
       "      <td>thanks in advance for the help i am writing a paper and for the life of me can t remember the proper term for a model that works as follows rawdata model outputmodel model outputmodel more specifi...</td>\n",
       "      <td>(machine learning, regression)</td>\n",
       "      <td>(regression,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>i am trying to understand the coefficients retrieved from running auto arima in r on my monthly time series of the annual change in house prices when doing so i obtain the following outcome series...</td>\n",
       "      <td>(r, time series)</td>\n",
       "      <td>(r, time series)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>let f y prod i binom n i y i pi x i y i pi x i n i y i where pi x i frac e sum j x ij b j e sum j x ij b j then the likelihood is l propto prod i pi x i y i pi x i n i y i l sum i y i log frac pi ...</td>\n",
       "      <td>(logistic, regression)</td>\n",
       "      <td>(regression, self study)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1192</th>\n",
       "      <td>i recently received the following question via email i ll post an answer below but i was interested to hear what others thought would you call logistic regression a non parametric test my understa...</td>\n",
       "      <td>(hypothesis testing, logistic)</td>\n",
       "      <td>(logistic,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>the background first i am currently working on some predictive modelling of some client shopping data to see if it is possible to categorise clients into one of nine ordinal categories according t...</td>\n",
       "      <td>(classification, machine learning)</td>\n",
       "      <td>(classification, machine learning)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>begin eqnarray e left left left x k right right left x k right right e left k left left x k right right right e left x left left x k right right right end eqnarray k is a constant and x is a rando...</td>\n",
       "      <td>(probability, self study)</td>\n",
       "      <td>(distributions, probability, self study)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1882</th>\n",
       "      <td>suppose there is a vector v that contains the body height of every person over years old on earth it looks something like this and is normally distributed also suppose there is simple function f t...</td>\n",
       "      <td>(distributions, hypothesis testing)</td>\n",
       "      <td>(distributions, probability)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>i have a record of one climate variable with a data point every year and another one which has sample spacing that varies between and years i even have a few ages in that series for which i have t...</td>\n",
       "      <td>(regression, time series)</td>\n",
       "      <td>(r, regression)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>i m trying to analyse the effect of smoking on health the health variable is binary healthy or not but there are different exposures to smoking active variable a and passive variable b variable a ...</td>\n",
       "      <td>(hypothesis testing, r)</td>\n",
       "      <td>(r, regression)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                      comment  ...                               predictions\n",
       "780   is there any function for m estimation in multivariate linear regression model in r i can estimate the beta s in my model by using the rlm by rewriting the y variables into one column but i would ...  ...                           (r, regression)\n",
       "1554  thanks in advance for the help i am writing a paper and for the life of me can t remember the proper term for a model that works as follows rawdata model outputmodel model outputmodel more specifi...  ...                             (regression,)\n",
       "823   i am trying to understand the coefficients retrieved from running auto arima in r on my monthly time series of the annual change in house prices when doing so i obtain the following outcome series...  ...                          (r, time series)\n",
       "445   let f y prod i binom n i y i pi x i y i pi x i n i y i where pi x i frac e sum j x ij b j e sum j x ij b j then the likelihood is l propto prod i pi x i y i pi x i n i y i l sum i y i log frac pi ...  ...                  (regression, self study)\n",
       "1192  i recently received the following question via email i ll post an answer below but i was interested to hear what others thought would you call logistic regression a non parametric test my understa...  ...                               (logistic,)\n",
       "1918  the background first i am currently working on some predictive modelling of some client shopping data to see if it is possible to categorise clients into one of nine ordinal categories according t...  ...        (classification, machine learning)\n",
       "616   begin eqnarray e left left left x k right right left x k right right e left k left left x k right right right e left x left left x k right right right end eqnarray k is a constant and x is a rando...  ...  (distributions, probability, self study)\n",
       "1882  suppose there is a vector v that contains the body height of every person over years old on earth it looks something like this and is normally distributed also suppose there is simple function f t...  ...              (distributions, probability)\n",
       "1059  i have a record of one climate variable with a data point every year and another one which has sample spacing that varies between and years i even have a few ages in that series for which i have t...  ...                           (r, regression)\n",
       "762   i m trying to analyse the effect of smoking on health the health variable is binary healthy or not but there are different exposures to smoking active variable a and passive variable b variable a ...  ...                           (r, regression)\n",
       "\n",
       "[10 rows x 3 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K868o9U3H5u0"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xG_KIc1xhl2V"
   },
   "outputs": [],
   "source": [
    "def predict_tag(comment):  \n",
    "  text=[]\n",
    "\n",
    "  #preprocess  \n",
    "  text = [cleaner(comment)]\n",
    "\n",
    "  #convert to integer sequences\n",
    "  seq = x_tokenizer.texts_to_sequences(text)\n",
    "\n",
    "  #pad the sequence\n",
    "  pad_seq = pad_sequences(seq,  padding='post', maxlen=max_len)\n",
    "\n",
    "  #make predictions\n",
    "  pred_prob = model.predict(pad_seq)\n",
    "  classes = classify(pred_prob,opt)[0]\n",
    "  \n",
    "  classes = np.array([classes])\n",
    "  classes = mlb.inverse_transform(classes)  \n",
    "  return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "0Al3kfEgEYhU",
    "outputId": "ab99c30c-5511-4ea1-b595-f082e33ea7db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment: For example, in the case of logistic regression, the learning function is a Sigmoid function that tries to separate the 2 classes\n",
      "Predicted Tags: [('classification', 'logistic', 'machine learning', 'regression')]\n"
     ]
    }
   ],
   "source": [
    "comment = \"For example, in the case of logistic regression, the learning function is a Sigmoid function that tries to separate the 2 classes\"\n",
    "\n",
    "print(\"Comment:\",comment)\n",
    "print(\"Predicted Tags:\",predict_tag(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kUSrb3nE-r6S"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "auto_tagging_system_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
