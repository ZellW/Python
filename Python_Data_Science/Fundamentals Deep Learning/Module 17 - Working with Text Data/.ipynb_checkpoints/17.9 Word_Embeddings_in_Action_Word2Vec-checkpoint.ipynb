{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZWWH8Gvgd09_"
   },
   "source": [
    "# Word Embeddings in Action - Word2Vec\n",
    "\n",
    "Steps to follow:\n",
    "\n",
    "1. Get data\n",
    "2. Clean text data\n",
    "3. Tokenization\n",
    "4. Prepare vocabulary\n",
    "5. Download pre-trained embeddings\n",
    "6. Get word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gPnpxm2s7Zty"
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P7ubotsYxoTi"
   },
   "source": [
    "# 1. Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AgII8L1Xd0-Q"
   },
   "outputs": [],
   "source": [
    "#input text\n",
    "text=['Building some bots for Wikipedia.',\n",
    "      'Wikipedia is flooded with information.',\n",
    "      'There is an app for everthing.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ndLOg1M3xw5u"
   },
   "source": [
    "# 2. Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2DpVJTY4uzCM"
   },
   "outputs": [],
   "source": [
    "# cleaning\n",
    "import re\n",
    "\n",
    "def clean(text):\n",
    "  #lower case\n",
    "  text=text.lower()\n",
    "  \n",
    "  #remove punctuations\n",
    "  text=re.sub('[^a-zA-Z]',\" \",text)\n",
    "  \n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9W8UmlIZu3Bq"
   },
   "outputs": [],
   "source": [
    "#call the clean function\n",
    "cleaned_text=[]\n",
    "\n",
    "for i in text:\n",
    "  cleaned_text.append(clean(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NSslu0ObyCxo"
   },
   "source": [
    "# 3. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "HF8vhjlDu4Y-",
    "outputId": "56841446-2e6e-4b22-c6ea-e4dd18f67628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['building', 'some', 'bots', 'for', 'wikipedia'], ['wikipedia', 'is', 'flooded', 'with', 'information'], ['there', 'is', 'an', 'app', 'for', 'everthing']]\n"
     ]
    }
   ],
   "source": [
    "#tokenize the text\n",
    "tokens=[]\n",
    "\n",
    "for i in cleaned_text:\n",
    "  tokens.append(i.split())\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OBcFBdRFyGzQ"
   },
   "source": [
    "# 4. Vocabulary Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UNd7ic_WvFqO",
    "outputId": "e451bf84-7c66-4b2f-802b-09c9e023dd58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['an', 'flooded', 'everthing', 'for', 'bots', 'with', 'information', 'is', 'there', 'wikipedia', 'app', 'building', 'some']\n"
     ]
    }
   ],
   "source": [
    "#construct vocabulary\n",
    "vocab=[]\n",
    "\n",
    "for i in tokens:\n",
    "  for j in i:\n",
    "    if j not in vocab:\n",
    "      vocab.append(j)\n",
    "\n",
    "#remove duplicate token\n",
    "vocab = list(set(vocab))\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RMc9TTmSyO67"
   },
   "source": [
    "#5. Feature Representation (word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Fpt6-ned1AI"
   },
   "source": [
    "### Download Google's pre-trained Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "WQNZ7Exh3jky",
    "outputId": "171824bb-d587-4c62-804f-fcb868906344"
   },
   "outputs": [],
   "source": [
    "# download and extract word2vec embeddings \n",
    "\n",
    "#! wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
    "#! gunzip GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since wget and gunzip are Linux utilities, this is the process I went through:\n",
    "\n",
    "- Download the file and copy to D:\\Large Filkes\\Analytics_Vidhya\n",
    "    - url = \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
    "- Used 7-Zip to decompress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! conda install -c conda-forge gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GoHGUSuId1AO"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# path of the downloaded model\n",
    "# filename = 'GoogleNews-vectors-negative300.bin'\n",
    "filename = \"../../../../../LargeData/Analytics_Vidhya/GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "# load into gensim\n",
    "w2vec = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-5WyFsVHd1Aj"
   },
   "source": [
    "Once you have executed the above code, your word2vec embeddings are finally installed and loaded. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "assnsO6Ld1Ap"
   },
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Please note that the length of every vector of the pre-trained word2vec embeddings is 300.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xV0yy-P0sPKf"
   },
   "outputs": [],
   "source": [
    "# empty array of shape (no. of tokens (30) X 300) to store word2vec features\n",
    "wordvec_array = np.zeros((len(vocab), 300))\n",
    "\n",
    "for i,j in enumerate(vocab):\n",
    "  wordvec_array[i,:] = w2vec.word_vec(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "WpsWAoaap5AA",
    "outputId": "d8a0dab2-8b49-4941-c3e2-238f40c8cdb2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.12597656,  0.19042969,  0.06982422, ...,  0.0612793 ,\n",
       "         0.17285156, -0.07861328],\n",
       "       [ 0.21679688, -0.03344727,  0.046875  , ..., -0.11914062,\n",
       "        -0.09521484,  0.02612305],\n",
       "       [ 0.22460938, -0.06542969, -0.08544922, ..., -0.14257812,\n",
       "         0.04394531,  0.03271484],\n",
       "       ...,\n",
       "       [ 0.11572266, -0.29101562, -0.30664062, ..., -0.24609375,\n",
       "        -0.17773438,  0.16113281],\n",
       "       [-0.00976562,  0.02856445,  0.05419922, ..., -0.01300049,\n",
       "         0.11621094,  0.02819824],\n",
       "       [ 0.17871094,  0.09130859, -0.00165558, ...,  0.125     ,\n",
       "         0.08056641,  0.01672363]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JRbv0MVl_hG1"
   },
   "source": [
    "Random Notes:\n",
    "\n",
    "- Word embeddings capture the context of the text and is obtained using the NN model. The size of embedding is not equal to the size of the vocabulary size.\n",
    "- Suppose you are learning word embedding for a vocabulary with 50000 words. Then the word vectors must be 50000 dimensional, in order to capture the full range of variations and meanings in those words.\n",
    "    - False: The size of the word-vector is significantly smaller than the size of vocabulary. Generally between 50 to 40O."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Word_Embeddings_in_Action_Word2Vec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
